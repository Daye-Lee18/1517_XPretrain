{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "\n",
    "import time \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPTokenizerFast\n",
    "\n",
    "from src.datasets.dataset_video_retrieval import (\n",
    "    HDVILAVideoRetrievalDataset, VideoRetrievalCollator)\n",
    "\n",
    "from src.configs.config import shared_configs\n",
    "from src_emotion.utils.misc import set_random_seed\n",
    "from src_emotion.utils.logger import LOGGER\n",
    "from src_emotion.utils.load_save import load_state_dict_with_mismatch\n",
    "from src.utils.emotion_utils import encode_query\n",
    "\n",
    "from src_emotion.utils.metrics import cal_cossim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "* Not using havarod library but cuda\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_video_ret_dataloader(dataset_name, vis_format, anno_path, vis_dir, cfg, tokenizer, mode):\n",
    "    is_train = mode == \"train\"\n",
    "    dataset = HDVILAVideoRetrievalDataset(\n",
    "        cfg=cfg,\n",
    "        vis_dir=vis_dir,\n",
    "        anno_path=anno_path,\n",
    "        vis_format=vis_format,\n",
    "        mode=mode\n",
    "    )\n",
    "    LOGGER.info(f\"[{dataset_name}] is_train {is_train} \"\n",
    "                f\"dataset size {len(dataset)}, \")\n",
    "\n",
    "    batch_size = cfg.train_batch_size if is_train else cfg.test_batch_size\n",
    "    vret_collator = VideoRetrievalCollator(\n",
    "        tokenizer=tokenizer, max_length=cfg.max_txt_len, is_train=is_train)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=cfg.n_workers,\n",
    "                            pin_memory=cfg.pin_mem,\n",
    "                            collate_fn=vret_collator.collate_batch)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def setup_model(cfg, device=None):\n",
    "    LOGGER.info(\"Setup model...\")\n",
    "    \n",
    "    if cfg.is_embed:\n",
    "        from src_emotion.modeling.VidCLIP import VidCLIP\n",
    "        model = VidCLIP(cfg)\n",
    "    else:\n",
    "        from src.modeling.VidCLIP import VidCLIP\n",
    "        model = VidCLIP(cfg)\n",
    "\n",
    "    if cfg.e2e_weights_path:\n",
    "        LOGGER.info(f\"Loading e2e weights from {cfg.e2e_weights_path}\")\n",
    "        \n",
    "        load_state_dict_with_mismatch(model, cfg.e2e_weights_path)\n",
    "    \n",
    "    if hasattr(cfg, \"overload_logit_scale\"):\n",
    "        model.overload_logit_scale(cfg.overload_logit_scale)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    LOGGER.info(\"Setup model done!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments\n",
    "\n",
    "* script 파일(.py)는 dummy script일 뿐이라서 아래의 config 에서 활용되지 않음. 즉, 무시해도 됨.\n",
    "* script 파일(.py) 이후의 arguments 들만 아래의 셀들에서 활용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/27/2024 15:04:20 - INFO - __main__ -   [demo-test] is_train False dataset size 21, \n",
      "03/27/2024 15:04:20 - INFO - __main__ -   [demo-test] is_train False dataset size 21, \n"
     ]
    }
   ],
   "source": [
    "set_random_seed(cfg_base.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# data for base model\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(cfg_base.clip_config)\n",
    "base_inference_loaders = {}\n",
    "for db in cfg_base.inference_datasets:\n",
    "    base_inference_loaders[db.name] = mk_video_ret_dataloader(\n",
    "        dataset_name=db.name, vis_format=db.vis_format,\n",
    "        anno_path=db.txt, vis_dir=db.vis,\n",
    "        cfg=cfg_base, tokenizer=tokenizer, mode=\"test\"\n",
    "    )\n",
    "\n",
    "# data for emotion embed model\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(cfg_embed.clip_config)\n",
    "embed_inference_loaders = {}\n",
    "for db in cfg_embed.inference_datasets:\n",
    "    embed_inference_loaders[db.name] = mk_video_ret_dataloader(\n",
    "        dataset_name=db.name, vis_format=db.vis_format,\n",
    "        anno_path=db.txt, vis_dir=db.vis,\n",
    "        cfg=cfg_embed, tokenizer=tokenizer, mode=\"test\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Base Model and Emotion Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/27/2024 15:04:21 - INFO - __main__ -   Setup model...\n",
      "Some weights of CLIPModel were not initialized from the model checkpoint at openai/clip-vit-base-patch32 and are newly initialized: ['vision_model.embeddings.added_cls', 'vision_model.embeddings.temporal_embedding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "03/27/2024 15:04:22 - INFO - __main__ -   Loading e2e weights from /data2/Hyejin/1517_XPretrain_HJ/CLIP-ViP/save_dir/IZMbMVUDqz/ckpt/model_best.pt\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   You can ignore the keys with `num_batches_tracked` or from task heads\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   Keys in loaded but not in model:\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   In total 0, []\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   Keys in model but not in loaded:\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   In total 0, []\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   Keys in model and loaded, but shape mismatched:\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   In total 0, []\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   Setup model done!\n",
      "03/27/2024 15:04:23 - INFO - __main__ -   Setup model...\n",
      "Some weights of CLIPModel were not initialized from the model checkpoint at openai/clip-vit-base-patch32 and are newly initialized: ['vision_model.embeddings.added_cls', 'text_model.embeddings.emotion_embedding.weight', 'vision_model.embeddings.temporal_embedding']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "03/27/2024 15:04:24 - INFO - __main__ -   Loading e2e weights from /data2/Hyejin/1517_XPretrain_HJ/save_dir/OUrtLkrKeh/ckpt/model_best.pt\n",
      "03/27/2024 15:04:25 - INFO - __main__ -   You can ignore the keys with `num_batches_tracked` or from task heads\n",
      "03/27/2024 15:04:25 - INFO - __main__ -   Keys in loaded but not in model:\n",
      "03/27/2024 15:04:25 - INFO - __main__ -   In total 0, []\n",
      "03/27/2024 15:04:25 - INFO - __main__ -   Keys in model but not in loaded:\n",
      "03/27/2024 15:04:25 - INFO - __main__ -   In total 0, []\n",
      "03/27/2024 15:04:25 - INFO - __main__ -   Keys in model and loaded, but shape mismatched:\n",
      "03/27/2024 15:04:25 - INFO - __main__ -   In total 0, []\n",
      "03/27/2024 15:04:25 - INFO - __main__ -   Setup model done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VidCLIP(\n",
       "  (clipmodel): CLIPModel(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (position_embedding): Embedding(77, 512)\n",
       "        (emotion_embedding): Embedding(8, 512)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionViPEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "        (position_embedding): Embedding(50, 768)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# base model\n",
    "base_model = setup_model(cfg_base, device=device)\n",
    "base_model.eval()\n",
    "\n",
    "# emotion embed model\n",
    "embed_model = setup_model(cfg_embed, device=device)\n",
    "embed_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module src.modeling.VidCLIP:\n",
      "\n",
      "forward(video, text_input_ids, text_input_mask, image=None, caption_ids=None, caption_masks=None) method of src.modeling.VidCLIP.VidCLIP instance\n",
      "    video [B, n_clips*num_frms, C, H, W]\n",
      "    text_input_ids [B, L]\n",
      "    text_input_mask [B, L]\n",
      "    image [B, img_num, C, H, W]\n",
      "    caption_ids [B, img_num, L]\n",
      "    caption_masks [B, img_num, L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(base_model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method forward in module src_emotion.modeling.VidCLIP:\n",
      "\n",
      "forward(video, text_input_ids, text_input_mask, emotions, image=None, caption_ids=None, caption_masks=None) method of src_emotion.modeling.VidCLIP.VidCLIP instance\n",
      "    video [B, n_clips*num_frms, C, H, W]\n",
      "    text_input_ids [B, L]\n",
      "    text_input_mask [B, L]\n",
      "    image [B, img_num, C, H, W]\n",
      "    caption_ids [B, img_num, L]\n",
      "    caption_masks [B, img_num, L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(embed_model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_demo(model, val_loaders, cfg):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    for loader_name, val_loader in val_loaders.items():\n",
    "        # print(f\"Loop val_loader {loader_name}.\")\n",
    "        valid_len = len(val_loader.dataset)\n",
    "        text_feats = []\n",
    "        vis_feats = []\n",
    "        vis_ids = []\n",
    "        for val_step, batch in enumerate(val_loader):\n",
    "            vis_ids.extend(batch.pop('vis_id')) # except vis_id\n",
    "            batch['video'] = batch['video'].to(device)\n",
    "            batch['text_input_ids'] = batch['text_input_ids'].to(device)\n",
    "            batch['text_input_mask'] = batch['text_input_mask'].to(device)\n",
    "            if cfg.is_embed:\n",
    "                batch['emotions'] = batch['emotions'].to(device)\n",
    "            else:\n",
    "                del batch['emotions']\n",
    "            \n",
    "            feats = model(**batch)  # dict\n",
    "            # print('feats vis_features', feats['vis_features'].shape)\n",
    "            vis_feat = feats['vis_features']\n",
    "            text_feat = feats['text_features']\n",
    "\n",
    "            # print('allgather vis_features', vis_feat.shape)\n",
    "\n",
    "            text_feats.append(text_feat.cpu().numpy())\n",
    "            vis_feats.append(vis_feat.cpu().numpy())\n",
    "\n",
    "        text_feats = np.vstack(text_feats)\n",
    "        vis_feats = np.vstack(vis_feats)\n",
    "\n",
    "        text_feats = text_feats[:valid_len]\n",
    "        vis_feats = vis_feats[:valid_len]\n",
    "        \n",
    "        sim_matrix = cal_cossim(text_feats, vis_feats)\n",
    "        sorted_score = sorted(sim_matrix[0], reverse = True)\n",
    "        ranks_idx = [sorted_score.index(s) for s in sim_matrix[0]] \n",
    "        happy_idx=[]\n",
    "        angry_idx=[]\n",
    "        for idx, vis_id in enumerate(vis_ids):\n",
    "            if 'happy' in vis_id:\n",
    "                happy_idx.append(idx)\n",
    "            elif 'angry' in vis_id:\n",
    "                angry_idx.append(idx)\n",
    "\n",
    "        happy_score_lst = [sim_matrix[0][idx] for idx in happy_idx]\n",
    "        angry_score_lst = [sim_matrix[0][idx] for idx in angry_idx]\n",
    "        happy_score = sum(happy_score_lst)/len(happy_score_lst)\n",
    "        angry_score = sum(angry_score_lst)/len(angry_score_lst)\n",
    "        \n",
    "        ranks = [vis_ids[idx] for idx in ranks_idx]\n",
    "        print(\"Video Ranks based on the Cosine Similarity Score: \")\n",
    "        print(ranks)\n",
    "        print(\"\\nHappy score mean: %f\\n\"%happy_score)\n",
    "        print(\"Angry score mean: %f\\n\"%angry_score)\n",
    "        return ranks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "#### text query: \"Happy smiling people\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['src/tasks/run_video_retrieval.py', '--config', '/data2/Hyejin/1517_XPretrain/CLIP-ViP/src/configs/demo/demo_retrieval_vip_base_32.json']\n",
    "cfg_base = shared_configs.parse_args()\n",
    "cfg_base.e2e_weights_path = '/data2/Hyejin/1517_XPretrain_HJ/CLIP-ViP/save_dir/IZMbMVUDqz/ckpt/model_best.pt' # load baseline model\n",
    "cfg_base.is_embed = False\n",
    "cfg_base.is_demo = True\n",
    "cfg_base.query = \"Happy smiling people\"\n",
    "cfg_base.emotion = encode_query(cfg_base.query, None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Happy smiling people\n",
      "\n",
      "Video Ranks based on the Cosine Similarity Score: \n",
      "['angry8', 'happy1', 'happy4', 'happy5', 'angry1', 'angry6', 'angry7', 'happy2', 'angry2', 'happy6', 'happy10', 'angry5', 'happy8', 'happy11', 'angry10', 'happy9', 'angry3', 'happy3', 'angry9', 'angry4', 'happy7']\n",
      "\n",
      "Happy score mean: 0.210959\n",
      "\n",
      "Angry score mean: 0.166257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "base_ranks = infer_demo(base_model, base_inference_loaders, cfg_base)\n",
    "video_dir = cfg_base.inference_datasets[0].vis\n",
    "\n",
    "print(\"Top 3 Selected Videos for Query [%s]\" %cfg_base.query)\n",
    "\n",
    "base_video_list = []\n",
    "for video in base_ranks[:3]:\n",
    "    base_video_list.append(os.path.join(video_dir, video+'.mp4'))\n",
    "\n",
    "for idx, video_path in enumerate(base_video_list): \n",
    "    print(\"\\nRank %d: %s\"%((idx+1),base_ranks[idx]))\n",
    "    display(Video(video_path, embed=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Embedded Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "### Text query: \"Happy smiling people\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['/data2/Hyejin/1517_XPretrain/CLIP-ViP/run_video_retrieval_hj_demo_ws.py', '--config', '/data2/Hyejin/1517_XPretrain/CLIP-ViP/src/configs/demo/demo_retrieval_vip_base_32.json']\n",
    "cfg_embed = shared_configs.parse_args()\n",
    "cfg_embed.is_embed = True\n",
    "cfg_embed.is_demo = True\n",
    "cfg_embed.query = \"Happy smiling people\"\n",
    "cfg_embed.emotion = encode_query(cfg_embed.query, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Happy smiling people\n",
      "\n",
      "Video Ranks based on the Cosine Similarity Score: \n",
      "['happy8', 'happy4', 'angry2', 'happy5', 'happy7', 'angry1', 'happy3', 'angry7', 'angry10', 'angry6', 'happy9', 'happy6', 'happy1', 'happy11', 'angry9', 'angry5', 'happy10', 'angry8', 'angry3', 'angry4', 'happy2']\n",
      "\n",
      "Happy score mean: 0.155641\n",
      "\n",
      "Angry score mean: 0.144195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embed_ranks = infer_demo(embed_model, embed_inference_loaders, cfg_embed)\n",
    "print(\"Top 3 Selected Videos for Query [%s]\" %cfg_embed.query)\n",
    "\n",
    "embed_video_list = []\n",
    "for video in embed_ranks[:3]:\n",
    "    embed_video_list.append(os.path.join(video_dir, video+'.mp4'))\n",
    "\n",
    "for idx, video_path in enumerate(embed_video_list): \n",
    "    print(\"\\nRank %d: %s\"%((idx+1),embed_ranks[idx]))\n",
    "    display(Video(video_path, embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE Embedding Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-sne visualization\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/.local/lib/python3.7/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81328/2679508683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# embed model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0membed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0membed_vis_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vis_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0membed_text_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/Hyejin/1517_XPretrain/CLIP-ViP/src_emotion/modeling/VidCLIP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, video, text_input_ids, text_input_mask, emotions, image, caption_ids, caption_masks)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;34m\"pixel_values\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \"return_loss\": False}\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclipmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_features\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_embeds\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/Hyejin/1517_XPretrain/CLIP-ViP/src_emotion/modeling/CLIP_ViP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, emotions, return_loss, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1164\u001b[0m         )\n\u001b[1;32m   1165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/Hyejin/1517_XPretrain/CLIP-ViP/src_emotion/modeling/CLIP_ViP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify pixel_values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_layrnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data2/Hyejin/1517_XPretrain/CLIP-ViP/src_emotion/modeling/CLIP_ViP.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mtime_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporal_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mpatch_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mpatch_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# [B*T, H*W, C]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "\n",
    "vis_ids = []\n",
    "embed_vis_feats = []\n",
    "embed_text_feats = []\n",
    "base_vis_feats = []\n",
    "base_text_feats = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in base_inference_loaders['demo-test']:\n",
    "        vis_ids.extend(data.pop('vis_id'))\n",
    "        \n",
    "        # embed model\n",
    "        embed_output = embed_model(**data)\n",
    "        embed_vis_feats.append(embed_output['vis_features'].cpu().numpy())\n",
    "        embed_text_feats.append(embed_output['text_features'].cpu().numpy())\n",
    "        \n",
    "        # base model\n",
    "        del data['emotions']\n",
    "        base_output = base_model(**data)\n",
    "        base_vis_feats.append(base_output['vis_features'].cpu().numpy())\n",
    "        base_text_feats.append(base_output['text_features'].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_vis_feats = np.vstack(embed_vis_feats)\n",
    "embed_text_feats = np.vstack(embed_text_feats)\n",
    "\n",
    "base_vis_feats = np.vstack(base_vis_feats)\n",
    "base_text_feats = np.vstack(base_text_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=5)\n",
    "\n",
    "embed_feats = np.vstack([embed_vis_feats, embed_text_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_labels = ['happy' if 'happy' in i else 'angry' for i in vis_ids]\n",
    "vis_labels\n",
    "# select angry index\n",
    "angry_idx = [i for i, label in enumerate(vis_labels) if label == 'angry']\n",
    "happy_idx = [i for i, label in enumerate(vis_labels) if label == 'happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize\n",
    "embed_tsne = tsne.fit_transform(embed_feats)\n",
    "feat_len = len(embed_vis_feats)\n",
    "\n",
    "vis_tsne = embed_tsne[:len(embed_vis_feats)]\n",
    "text_tsne = embed_tsne[len(embed_vis_feats):]\n",
    "plt.scatter(vis_tsne[angry_idx, 0], vis_tsne[angry_idx, 1], c='r', marker='*', label='angry video')\n",
    "plt.scatter(vis_tsne[happy_idx, 0], vis_tsne[happy_idx, 1], c='b', marker='*', label='happy video')\n",
    "plt.scatter(text_tsne[angry_idx, 0], text_tsne[angry_idx, 1], c='r', marker='o', label='angry text')\n",
    "plt.scatter(text_tsne[happy_idx, 0], text_tsne[happy_idx, 1], c='b', marker='o', label='happy text')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mie1517h-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
