# CLIP-ViP

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/clip-vip-adapting-pre-trained-image-text/video-retrieval-on-activitynet)](https://paperswithcode.com/sota/video-retrieval-on-activitynet?p=clip-vip-adapting-pre-trained-image-text)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/clip-vip-adapting-pre-trained-image-text/video-retrieval-on-didemo)](https://paperswithcode.com/sota/video-retrieval-on-didemo?p=clip-vip-adapting-pre-trained-image-text)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/clip-vip-adapting-pre-trained-image-text/video-retrieval-on-lsmdc)](https://paperswithcode.com/sota/video-retrieval-on-lsmdc?p=clip-vip-adapting-pre-trained-image-text)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/clip-vip-adapting-pre-trained-image-text/video-retrieval-on-msr-vtt-1ka)](https://paperswithcode.com/sota/video-retrieval-on-msr-vtt-1ka?p=clip-vip-adapting-pre-trained-image-text)

This repo is the official pytorch implementation of "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language
Representation Alignment". 

The code and related instructions will be released soon. Thanks for your patience.
