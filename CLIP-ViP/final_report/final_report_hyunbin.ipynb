{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP-ViP Modified Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader\n",
    "In order for the model to utilize the emotion data that we extracted from the text data, we need to modify the Dataset class that is used to call this newly added data. Below is the modified version of the HDVILAVideoRetrievalDataset class. We mainly update the \\_\\_getitem\\_\\_ method to additionally read the emotion data as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from src.utils.logger import LOGGER\n",
    "from src.utils.basic_utils import flat_list_of_lists\n",
    "from src.datasets.data_utils import mask_batch_text_tokens, img_collate\n",
    "from src.datasets.dataloader import init_transform_dict, init_transform_dict_simple\n",
    "import decord # video loader \n",
    "from decord import VideoReader\n",
    "from decord import cpu, gpu\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import lmdb\n",
    "import glob\n",
    "import src.utils.stop_words as stop_words\n",
    "from PIL import Image\n",
    "from src.datasets.sample_frames import SampleFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDVILAVideoRetrievalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    datalist\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, vis_dir, anno_path, vis_format='video', mode=\"train\"):\n",
    "        assert vis_format in [\"video\", \"frame\"]\n",
    "        self.cfg = cfg\n",
    "        self.vis_dir = vis_dir\n",
    "        self.anno_path = anno_path\n",
    "        self.mode = mode\n",
    "        self.vis_format = vis_format\n",
    "        self.n_clips = cfg.train_n_clips if mode == \"train\" else cfg.test_n_clips\n",
    "        self.num_frm = cfg.train_num_frms if mode == \"train\" else cfg.test_num_frms\n",
    "        self.sample_rate = cfg.sample_rate\n",
    "        if hasattr(cfg, \"text_pos_num\"):\n",
    "            self.pos_num = cfg.pos_num\n",
    "        else:\n",
    "            self.pos_num = 1\n",
    "        self.transform = init_transform_dict_simple(video_res=cfg.video_res,\n",
    "                                             input_res=cfg.input_res)[mode]\n",
    "        self.frame_sampler = SampleFrames(clip_len=self.num_frm, \n",
    "                                          frame_interval=self.sample_rate, \n",
    "                                          num_clips=self.n_clips, \n",
    "                                          temporal_jitter=True)\n",
    "        self.init_dataset_process()\n",
    "\n",
    "\n",
    "    def init_dataset_process(self):\n",
    "        json_type = os.path.splitext(self.anno_path)[-1]\n",
    "        assert json_type in ['.json', '.jsonl']\n",
    "\n",
    "        if json_type == '.jsonl':\n",
    "            data = []\n",
    "            with open(self.anno_path) as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line))\n",
    "        else:\n",
    "            data = json.load(open(self.anno_path))\n",
    "        self.datalist = data\n",
    "        if self.cfg.is_demo:\n",
    "            self.dir_list = os.listdir(self.vis_dir)\n",
    "\n",
    "    def id2path(self, id):\n",
    "        clip_name = id\n",
    "        if self.vis_format == 'video':\n",
    "            name = os.path.join(self.vis_dir, clip_name.split('/')[-1]+\".mp4\")\n",
    "            if \"lsmdc\" in self.vis_dir:\n",
    "                name = os.path.join(self.vis_dir, clip_name + \".avi\")\n",
    "        else:\n",
    "            name = os.path.join(self.vis_dir, clip_name)\n",
    "        return name\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.cfg.is_demo:\n",
    "            return len(self.dir_list)\n",
    "        else:\n",
    "            return len(self.datalist)\n",
    "\n",
    "    def get_sample_idx(self, total_frame_num):\n",
    "        \"\"\"\n",
    "        sample rate > 0: use SampleFrames, loop default\n",
    "        sample rate = 0: uniform sampling, temporal jittering\n",
    "        \"\"\"\n",
    "        if self.sample_rate > 0:\n",
    "            results = {\"total_frames\": total_frame_num,\n",
    "                    \"start_index\": 0}\n",
    "            results = self.frame_sampler(results)\n",
    "            return results[\"frame_inds\"]\n",
    "        elif self.sample_rate == 0:\n",
    "            if hasattr(self.cfg, \"sample_jitter\") and self.cfg.sample_jitter and self.mode == \"train\":\n",
    "                interval = int(total_frame_num / (self.n_clips*self.num_frm - 1))\n",
    "                start = np.random.randint(0, interval+1)\n",
    "                end = np.random.randint(total_frame_num-1-interval, total_frame_num)\n",
    "                return np.linspace(start, end, self.n_clips*self.num_frm).astype(int)\n",
    "            else:\n",
    "                return np.linspace(0, total_frame_num-1, self.n_clips*self.num_frm).astype(int)\n",
    "\n",
    "    def load_video(self, vis_path):\n",
    "        vr = VideoReader(vis_path, ctx=cpu(0))\n",
    "        total_frame_num = len(vr)\n",
    "\n",
    "        frame_idx = self.get_sample_idx(total_frame_num)\n",
    "        img_array = vr.get_batch(frame_idx) # (n_clips*num_frm, H, W, 3)\n",
    "\n",
    "        img_array = img_array.permute(0, 3, 1, 2).float() / 255.\n",
    "        img_array = self.transform(img_array)\n",
    "\n",
    "        return img_array\n",
    "\n",
    "    def load_frames(self, vis_path, total_frame_num):\n",
    "        frame_idx = self.get_sample_idx(total_frame_num)\n",
    "\n",
    "        img_array = []\n",
    "        for i in frame_idx:\n",
    "            img = Image.open(os.path.join(vis_path, \\\n",
    "                    vis_path.split('/')[-1] + '_{0:03d}.jpg'.format(i))).convert(\"RGB\")\n",
    "            img_array.append(np.array(img))\n",
    "        img_array = torch.from_numpy(np.array(img_array))  # (n_clips*num_frm, H, W, 3)\n",
    "\n",
    "        img_array = img_array.permute(0, 3, 1, 2).float() / 255.\n",
    "        img_array = self.transform(img_array)\n",
    "\n",
    "        return img_array\n",
    "\n",
    "    # This is where we modify the code to include the emotion data\n",
    "    def __getitem__(self, index):\n",
    "        if self.cfg.dummy_data:\n",
    "            return dict(\n",
    "            video = torch.randn(self.n_clips*self.num_frm, 3, self.cfg.input_res[0], self.cfg.input_res[1]),  # [clips, num_frm, C, H_crop, W_crop]\n",
    "            texts = [\"This is a dummy sentence, which contains nothing meaningful.\"]\n",
    "        )\n",
    "\n",
    "        if self.cfg.is_demo:\n",
    "            # Get the list of all files and directories \n",
    "            # path = self.vis_dir\n",
    "            video = self.dir_list[index]\n",
    "            video_id, _ = os.path.splitext(video)\n",
    "            vis_id = video_id\n",
    "            texts = [self.cfg.query]  # for testing\n",
    "            emotions = self.cfg.emotion\n",
    "            \n",
    "        else:\n",
    "            if not (\"video_id\" in self.datalist[index].keys()):\n",
    "                video = self.datalist[index][\"video\"]\n",
    "                video_id, _ = os.path.splitext(video)\n",
    "                vis_id = video_id\n",
    "                texts = self.datalist[index]['caption']\n",
    "            else:\n",
    "                vis_id = self.datalist[index]['video_id']\n",
    "                texts = self.datalist[index]['caption']\n",
    "\n",
    "            if isinstance(texts, list):\n",
    "                texts = random.sample(self.datalist[index]['caption'], self.pos_num)\n",
    "                if 'didemo' in self.anno_path:\n",
    "                    texts = [' '.join(self.datalist[index]['caption'])]\n",
    "            else:\n",
    "                texts = [texts]\n",
    "\n",
    "            # We get the emotions from the datalist\n",
    "            emotions = [self.datalist[index][emotion] for emotion in [\"joy\", \"trust\", \"surprise\", \"anticipation\", \"fear\", \"sadness\", \"disgust\", \"anger\"]]\n",
    "            \n",
    "        vis_path = self.id2path(vis_id)\n",
    "        video = self.load_video(vis_path) if self.vis_format=='video' else self.load_frames(vis_path, self.datalist[index]['num_frame'])     \n",
    "\n",
    "        return dict(\n",
    "            video = video,  # [clips*num_frm, C, H_crop, W_crop]\n",
    "            texts = texts,\n",
    "            emotions = emotions,\n",
    "            vis_id = vis_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collator for creating batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom collator is used to create the batches of:\n",
    "* sequences of tokens \n",
    "* the attention masks corresponding to each sequence \n",
    "* videos\n",
    "* and now the emotions in each sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoRetrievalCollator(object):\n",
    "    def __init__(self, tokenizer, max_length=40, is_train=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        if isinstance(batch[0][\"video\"], torch.Tensor):\n",
    "            v_collate = default_collate\n",
    "        else:\n",
    "            v_collate = img_collate\n",
    "        video = v_collate([d[\"video\"] for d in batch])\n",
    "\n",
    "        text_examples = flat_list_of_lists([d[\"texts\"] for d in batch])\n",
    "        # Add emotion data\n",
    "        emotions = torch.LongTensor([(d[\"emotions\"]) for d in batch])\n",
    "        \n",
    "        # for vis_id collation\n",
    "        vid_collate = default_collate\n",
    "        vis_id = vid_collate([d[\"vis_id\"] for d in batch])\n",
    "        \n",
    "        text_str_list = [d for d in text_examples]  # (B, )\n",
    "\n",
    "        batch_enc = self.tokenizer.batch_encode_plus(\n",
    "            text_str_list,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        text_input_ids = batch_enc.input_ids  # (B, L)\n",
    "        text_input_mask = batch_enc.attention_mask  # (B, L)\n",
    "\n",
    "        # Add emotion data to the final returned batch\n",
    "        collated_batch = dict(\n",
    "            video=video,   # [B, clips, num_frm, C, H_crop, W_crop]\n",
    "            text_input_ids=text_input_ids,\n",
    "            text_input_mask=text_input_mask,\n",
    "            emotions=emotions,\n",
    "            vis_id=vis_id\n",
    "        )\n",
    "\n",
    "        return collated_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embeddings\n",
    "The following code shows our main contributions and the implementation of our ideas. The CLIP model is complicated and consists of a hierarchy of many classes. Largely, it consists of two transformers, each for learning the video and text data together. These transformers are further divided into smaller components like encoder and embedding classes. We start with the CLIPTextEmbeddings class, which we modify to incorporate the emotion data in the creation of the text embeddings. \n",
    "\n",
    "We do this by initializing an embedding for each emotion, resulting in a total of 8 emotions. Here, the dimensions of the embeddings are the same as the token and positional embeddings. For each sequence, which has a set of corresponding emotions, we call the embeddings for each emotion and then average them to create a single aggregated emotion embedding. This embedding is then added to each token embedding in the sequence alongside the positional embeddings. This allows the model to incorporate the emotion information extracted from each sequence (caption) when learning their representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTextEmbeddings(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n",
    "        self.emotion_embedding = nn.Embedding(8, embed_dim)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        emotions: Optional[torch.LongTensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n",
    "        batch_size = input_ids.shape[0] if input_ids is not None else inputs_embeds.shape[0]\n",
    "\n",
    "        if emotions is not None:\n",
    "            # Change non-zero values to 1, effectively binarizing the input\n",
    "            emotions = torch.where(emotions > 0, torch.ones_like(emotions), torch.zeros_like(emotions))\n",
    "        \n",
    "        # Retrieve all emotion embeddings\n",
    "        all_emotion_embeds = self.emotion_embedding.weight.unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size, 8, embed_dim]\n",
    "\n",
    "        if emotions is not None:\n",
    "            emotion_mask = emotions.unsqueeze(-1).type_as(all_emotion_embeds)  # [batch_size, 8, 1]\n",
    "            selected_emotion_embeds = all_emotion_embeds * emotion_mask  # [batch_size, 8, embed_dim]\n",
    "            emotion_embeds = selected_emotion_embeds.sum(1) / (emotion_mask.sum(1) + 1e-8)  # [batch_size, embed_dim]\n",
    "        else:\n",
    "            emotion_embeds = torch.zeros(batch_size, self.token_embedding.embedding_dim, device=input_ids.device if input_ids is not None else inputs_embeds.device)\n",
    "\n",
    "        emotion_embeds = emotion_embeds.unsqueeze(1).expand(-1, seq_length, -1)  # [batch_size, seq_length, embed_dim]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.token_embedding(input_ids)\n",
    "\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = inputs_embeds + position_embeddings + emotion_embeds\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIPTextTransformer\n",
    "Here we update the CLIPTextTransformer class to accept the emotion data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTextTransformer(nn.Module):\n",
    "    def __init__(self, config: CLIPTextConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        embed_dim = config.hidden_size\n",
    "        self.embeddings = CLIPTextEmbeddings(config)\n",
    "        self.encoder = CLIPEncoder(config)\n",
    "        self.final_layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n",
    "    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPTextConfig)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        emotions: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if input_ids is None:\n",
    "            raise ValueError(\"You have to specify either input_ids\")\n",
    "\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "\n",
    "        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids, emotions=emotions)\n",
    "\n",
    "        bsz, seq_len = input_shape\n",
    "        # CLIP's text model uses causal mask, prepare it here.\n",
    "        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n",
    "        if_fp16 = hidden_states.dtype == torch.float16\n",
    "        causal_attention_mask = self._build_causal_attention_mask(bsz, seq_len, fp16=if_fp16).to(hidden_states.device)\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, hidden_states.dtype)\n",
    "\n",
    "        encoder_outputs = self.encoder(\n",
    "            inputs_embeds=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            causal_attention_mask=causal_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        last_hidden_state = encoder_outputs[0]\n",
    "        last_hidden_state = self.final_layer_norm(last_hidden_state)\n",
    "\n",
    "        # text_embeds.shape = [batch_size, sequence_length, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        pooled_output = last_hidden_state[torch.arange(last_hidden_state.shape[0]), input_ids.argmax(dim=-1)]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=last_hidden_state,\n",
    "            pooler_output=pooled_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _build_causal_attention_mask(self, bsz, seq_len, fp16=False):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(bsz, seq_len, seq_len)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        mask = mask.unsqueeze(1)  # expand mask\n",
    "        if fp16:\n",
    "            mask = mask.half()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
