{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhgf7vZc1y6YONvtXtWzXQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Experimental Result"],"metadata":{"id":"z-n1pu3c_GMK"}},{"cell_type":"markdown","source":["To first evaluate the effects of training on different size datasplits, we trained the baseline model on the conventional 7k and 9k training sets and validated on the 1k validation set. In addition, we also conducted training on the 6k-emotion dataset we previously created. To evaluate the overall performance of the models, we first evaluate their performance on the entire test set, labeled \"Emotion+Neutral\" in the tables. On the other hand, __to evaluate the performance of these models on the retrieval of the caption-video pairs with emotions, which we previously identified as 34 videos out of the 1000 videos in the test dataset, we calculate the recall values for only these 34 queries, instead of the total 1000.__ These results are listed in the columns labeled \"Emotion\" in the tables."],"metadata":{"id":"2CbIm8Nv_D_I"}},{"cell_type":"markdown","source":["## MSR-VTT 7k: Baseline Model\n","\n","\n","| Test Data                 | Emotion+Neutral |  Emotion+Neutral          |  Emotion  | Emotion          |\n","|:---------------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n","| Metric                    | T2V          | V2T          | T2V          | V2T          |\n","| Recall@1               |   49.4000%   |  47.8044%    |  32.3529%  |  41.1765%  |\n","| Recall@5               |   73.0000%   |  74.9501%    |  61.7647%  |  67.6471%  |\n","| Recall@10              |   83.4000%   |   84.4311%   |  73.5294%  |  82.3529%  |\n","| Recall Median          |     2.0         |   2.0        |    3.5     |    3.0     |\n","| Recall Mean            |       14.5       |   10.3       |   18.1     |   14.6     |\n","\n","\n","\n"],"metadata":{"id":"rRU-HASK_R6-"}},{"cell_type":"markdown","metadata":{"id":"8fed732a"},"source":["## MSR-VTT 9k: Baseline Model\n","\n","\n","| Test Data                 | Emotion+Neutral |  Emotion+Neutral          |  Emotion  | Emotion          |\n","|:---------------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n","| Metric                    | T2V          | V2T          | T2V          | V2T          |\n","| Recall@1               |  49.5000%    | 49.3028%     |  35.2941%  |  35.2941%  |\n","| Recall@5               |   74.7000%   | 76.6932%     |  61.7647%  |  67.6471%  |\n","| Recall@10              |  84.8000%    | 85.3586%     |  73.5294%  |  82.3529%  |\n","| Recall Median          |   2.0        |    2.0       |    2.5     |    3.0     |\n","| Recall Mean            |   13.4       |    9.5       |   15.9     |   13.1     |\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0529c400"},"source":["## MSR-VTT 6k (Emotion): Baseline Model\n","| Test Data                 | Emotion+Neutral |  Emotion+Neutral          |  Emotion  | Emotion          |\n","|:---------------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n","| Metric                    | T2V          | V2T          | T2V          | V2T          |\n","| Recall@1               |   49.0000%   |  48.4032%    |  29.4118%  |  32.3529%  |\n","| Recall@5               |    73.2000%  |  75.6487%    |  58.8235%  |  70.5882%  |\n","| Recall@10              |  84.5000%    |  84.7305%    |  79.4118%  |  79.4118%  |\n","| Recall Median          |   2.0        |   2.0        |    3.5     |    2.0     |\n","| Recall Mean            |    13.6      |   9.9        |   16.8     |   14.7     |\n","\n"]},{"cell_type":"markdown","metadata":{"id":"39d0aa5a"},"source":["## MSR-VTT 6k (Emotion): Our Model (Emotion Embeddings)\n","| Test Data                 | Emotion+Neutral |  Emotion+Neutral          |  Emotion  | Emotion          |\n","|:---------------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n","| Metric                    | T2V          | V2T          | T2V          | V2T          |\n","| Recall@1                  |  23.9000%    |    44.8104%  | 5.8824%      | 8.8235%      |\n","| Recall@5                  |  41.5000%    |  28.1437%    | 14.7059%     | 20.5882%     |\n","| Recall@10                 |      49.00%  |   50.9980%   | 23.5294%     | 20.5882%     |\n","| Recall Median             |     11.0     |   9.5        | 83.0         | 66.5         |\n","| Recall Mean               |      124.4   |      72.6    | 206.6        | 115.5        |\n"]},{"cell_type":"markdown","metadata":{"id":"3129607c"},"source":["As expected, in regard to the model's performance on emotion-containing queries in the \"Emotion\" columns, the performance degrades on the 6k(emotion) dataset in comparison to the other two datasets with both emotional and neutral data, as the model sees less data during training. From this, we conclude that training exclusively on data containing only emotion does not translate to improved performance on emotion-containing queries. In addition, when comparing the results for the entire test set with only the emotion-containing queries, we find that for all training set sizes, the recall values for \"Emotion\" are all lower than for \"Emotion+Neutral\". We find these as reasons to additionally implement methods to focus on the emotion information within our data, to better find such matches."]},{"cell_type":"markdown","source":["\n","The experiment's findings indicate a decline in overall performance when contrasting the 'Baseline' with 'Ours'. Specifically, for 'Text to Video' (T2V), the R@1 metric fell sharply from 29.41% to 5.89%, and for 'Video to Text' (V2T), it decreased from 32.35% to 8.83%. A similar downward trend was observed in the R@5 metric, which dropped from 58.82% to 14.71% for T2V, and from 70.59% to 15.63% for V2T. The R@10 metric also saw a significant reduction, declining from 79.41% to 23.53% for T2V and from 79.41% to 20.59% for V2T."],"metadata":{"id":"zXAOVTdMCWQ2"}},{"cell_type":"markdown","source":[],"metadata":{"id":"YfwP7qi-EF3O"}},{"cell_type":"markdown","source":[],"metadata":{"id":"r3uiQIA7HZWg"}},{"cell_type":"markdown","source":["## Evaluation Metric"],"metadata":{"id":"g9Lt82x-DxxH"}},{"cell_type":"markdown","source":["**Recall@k**: Recall $(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}})$ of Top $ k $ samples\n","- Among the top  $ k $ samples, the number of samples that are actually similar to the query  $ \\text{TP} $ is divided by the total number of query samples $\\text{ TP + FN}$.\n"],"metadata":{"id":"UrvdsSULD01V"}},{"cell_type":"markdown","source":["### Example of Recall@k"],"metadata":{"id":"hL8SyEuAFQBh"}},{"cell_type":"markdown","source":["For the quantitative evaluation metric, we used the Recall@k metric, which is commonly used for the recommendation task.\n","Recall@k is to compute the recall between the Top k samples based on the ranking. Therefore, among the top k samples, the number of samples that actually correspond to the query(or video), so called True Positive, is divided by the total number of query(or video) samples.\n","For example of Recall@5, you can see in the figure of similarity matrix which is ranked by cosine similarity, that consists of 8 text and video pairs. If you look at the first row of the matrix, as we evaluate top 5 ranking, there is the corresponding video at the rank 3 which means true positive of that row becomes 1. Likewise compute R@5 for all the rows in the matrix, we can get the 6 TP divided by 8 of total samples, results in 75% of R@5 for the example matrix.\n","\n"],"metadata":{"id":"3AYVz1--Hy3x"}},{"cell_type":"markdown","source":["![recall@5](https://github.com/Hyejin3194/MIE1517_Project_Emotion-Text-to-Video-Retrieval/blob/main/images/recall@5.png?raw=true)"],"metadata":{"id":"NE4vKR11IUsf"}},{"cell_type":"markdown","source":["\n","Likewise the Recall@1, you can compute the socre with the top 1 ranking between all samples. There is only one True positive sample among 8 samples, so we can get 1 over 8 R@1 score that is 12.5%.\n"],"metadata":{"id":"V_AqibT1Icp5"}},{"cell_type":"markdown","source":["![recall@1](https://github.com/Hyejin3194/MIE1517_Project_Emotion-Text-to-Video-Retrieval/blob/main/images/recall@1.png?raw=true)\n","\n"],"metadata":{"id":"AplpXe2ZFajr"}},{"cell_type":"markdown","source":["##model training\n","To model training, you have to add the --is_train argument.\n","\n","In config file, insert pretrained clip model path to \"e2e_weights_path\".\n","\n","For the dataset, insert the text json file path to the \"txt\" in \"training_dataset\", \"val_datasets\", \"inference_dataset\".\n","\n","The argument --blob_mount_dir is for set the directory that can save the training log and checkpoint of the model.\n","\n","Also, you can adjust the learning rate, batch size, epochs, etc in the config file.\n","\n","\n","For the emotion embeddin model training, add the --is_embed argument to the command line.\n","\n","Example of command for training:\n","- Baseline model\n","python run_video_retrieval.py --config src/configs/msrvtt_retrieval/msrvtt_retrieval_vip_base_32.json --blob_mount_dir ./ --is_train\n","\n","- emotion embedding model\n","\n","python run_video_retrieval.py --config src/configs/msrvtt_retrieval/msrvtt_retrieval_vip_base_32.json --blob_mount_dir ./ --is_train --is_embed\n","\n","\n","## model test\n","To execute the code only for test, remove the --is_train argument and replace the \"e2e_weights_path\" with pretrained clip-vip model. And remove the --is_train argument in the command line.\n","\n","\n","Example of command for test:\n","- Baseline model\n","\n","python run_video_retrieval.py --config src/configs/msrvtt_retrieval/msrvtt_retrieval_vip_base_32.json --blob_mount_dir ./\n","- emotion embedding model\n","\n","python run_video_retrieval.py --config src/configs/msrvtt_retrieval/msrvtt_retrieval_vip_base_32.json --blob_mount_dir ./ --is_embed\n","\n","\n","\n","##model demo\n","To execute the code only for demo, remove the --is_train argument and replace the \"e2e_weights_path\" with pretrained clip-vip model. And remove the --is_train argument in the command line.\n","\n","\n","Example of command for demo:\n","- Baseline model\n","\n","python run_video_retrieval_demo.py --config src/configs/demo/demo_retrieval_vip_base_32.json --blob_mount_dir ./ --is_demo\n","- emotion embedding model --query \"happy smiling people\"\n","\n","python run_video_retrieval_demo.py --config src/configs/demo/demo_retrieval_vip_base_32.json --blob_mount_dir ./ --is_demo --query \"happy smiling people\""],"metadata":{"id":"zoiBbx_k1Pb8"}},{"cell_type":"code","source":[],"metadata":{"id":"4FJd0QfR1TYS"},"execution_count":null,"outputs":[]}]}