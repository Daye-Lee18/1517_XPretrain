{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28480dd3-e5a2-4964-bd2f-2a1640cfe882",
   "metadata": {},
   "source": [
    "# Project Objective \n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60134dd9-ba72-464a-827b-eafe8d6e8589",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35ef2b-c8a1-42a2-8645-8f6f1e5ca6ac",
   "metadata": {},
   "source": [
    "<img src=\"./baseline.png\" alt=\"nn\" width=\"1000\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50015e8-c8a8-40be-a3bd-1eb6ce43061c",
   "metadata": {},
   "source": [
    "# Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbc376-9b3d-48e4-992d-226ada60a46f",
   "metadata": {},
   "source": [
    "<img src=\"./ourEmbeddingModel_2.png\" alt=\"nn\" width=\"1000\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d7cc7-6358-4c3c-a5a4-25e89873f44a",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "    - Task: video-language alignment (Video-to-Text, Text-to-Video)\n",
    "    - Goal: \n",
    "        - Adapting image-text pre-trained models to video-text pre-training (i.e. post-training) \n",
    "        - Video Proxy mechanism on the basis of CLIP (CLIP-ViP) \n",
    "    - Approach \n",
    "        - In-domain auxiliary data generation: \n",
    "            - to bridge language domain gaps between images and videos datasets \n",
    "            - introduce auxiliary captions into large-scale video-subtitle data to reduce the language domain gap between pre-training and downstream data \n",
    "                - pre-training: Image-Text learning \n",
    "                - downstream data: Video-Text learning \n",
    "\n",
    "        - Video Proxy Mechanism:\n",
    "            - to enable the Vision Transformer (ViT) model for both image and video encoding \n",
    "            - Before feeding into CLIP, we concatenate path tokens with a set of learnable parameters called video proxy tokens\n",
    "            - The output of the first video proxy token will be regarded as the video's representation \n",
    "    - Loss function:\n",
    "        - info-NCE loss \n",
    "\n",
    "## Our Model with emotion embedding \n",
    "    - \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fbd65-33e7-49a9-843f-082caa3798f4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f198bc-3f50-4eba-968a-947db24cbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7410021-66f0-4b0f-b832-2df37fe742af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\n",
    "\"video\": \"video7112.mp4\",\n",
    "\"video_id\": \"video7112\",\n",
    "\"caption\": \"while other friends too try and hitting the basket another is eager to achieve his fourth successful basket in basketball\",\n",
    "\"duration\": 18.35,\n",
    "\"emotion\": 1,\n",
    "\"positive\": 4.0,\n",
    "\"negative\": 0.0,\n",
    "\"neutral\": 0.0,\n",
    "\"joy\": 4.0,\n",
    "\"trust\": 3.0,\n",
    "\"surprise\": 1.0,\n",
    "\"anticipation\": 3.0,\n",
    "\"fear\": 0.0,\n",
    "\"sadness\": 0.0,\n",
    "\"disgust\": 0.0,\n",
    "\"anger\": 0.0\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb501e29-89af-43ac-8888-0d9abb2f80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "emotion_embedding = nn.Embedding(8, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf737c3-85cb-48cb-b1d4-b1e76f604f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(8, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ddaa3de-04a8-43bb-9047-678829498ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "emotions: tensor([[1., 1., 1., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change non-zero values to 1, effectively binarizing the input\n",
    "emotions = torch.tensor(np.array([4.0, 3.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0])).unsqueeze(0) \n",
    "print(emotions.shape) # (bs, 8)\n",
    "if emotions is not None:\n",
    "    emotions = torch.where(emotions > 0, torch.ones_like(emotions), torch.zeros_like(emotions))\n",
    "    print(f\"emotions: {emotions}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9cc6678-47a9-4d5d-8d76-c8657b802794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all emotion embeddings\n",
    "batch_size = 1 \n",
    "all_emotion_embeds = emotion_embedding.weight.unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size, 8, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78b51f96-b1b6-4be9-a0d2-be1aae53e04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emotion_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b48b6362-506a-4306-a52f-afc1e57cf248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion_mask: tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]), shape = torch.Size([1, 8, 1])\n",
      "\n",
      "selected_motion_embeds: torch.Size([1, 8, 512])\n",
      "\n",
      "emotion_embeds.shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "if emotions is not None:\n",
    "    emotion_mask = emotions.unsqueeze(-1).type_as(all_emotion_embeds)  # [batch_size, 8, 1]\n",
    "    print(f\"emotion_mask: {emotion_mask}, shape = {emotion_mask.shape}\\n\")\n",
    "    selected_emotion_embeds = all_emotion_embeds * emotion_mask  # [batch_size, 8, embed_dim]\n",
    "    print(f\"selected_motion_embeds: {selected_emotion_embeds.shape}\\n\")\n",
    "    emotion_embeds = selected_emotion_embeds.sum(1) / (emotion_mask.sum(1) + 1e-8)  # [batch_size, embed_dim]\n",
    "\n",
    "    print(f\"emotion_embeds.shape: {emotion_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab9731-0ad7-43ef-baaf-d87a796d8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = ? \n",
    "\n",
    "emotion_embeds = emotion_embeds.unsqueeze(1).expand(-1, seq_length, -1)  # [batch_size, seq_length, embed_dim]\n",
    "\n",
    "position_embeddings = self.position_embedding(position_ids)\n",
    "\n",
    "# embedding with additional emotion_embeds \n",
    "embeddings = inputs_embeds + position_embeddings + emotion_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709cb46-fbe4-4621-9b43-174771c783ea",
   "metadata": {},
   "source": [
    "# Future Work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8407c-4d8b-4b33-a6c8-1e4177148b7a",
   "metadata": {},
   "source": [
    "1. Test Data Collection:\n",
    "    - Image-captioning model OFA-Caption (Wang et al., 2022b) to generate one caption for the middle frame of each video in HD-VILA-100M\n",
    "          - with max length of 16 words\n",
    "2. Model development\n",
    "    - 현재 우리 모델은 하나의 caption에 대한 nn.Embedding 결과를 단순히 aggregation (addition) 했는데, emotion embedding을 concat해서 fc layer를 통과하는 방식을 접목시킬 수 있을 것 같다.\n",
    "    - 새로운 emotion encoder를 만들어서, video, text, and emotion features 들의 새로운 cross attention을 시도해볼 수 있을 것 같다. (1) \n",
    "    - video 에서 emotion 정보를 추출하여 이또한 video encoder embedding에 추가하여 감정 정보를 반영한다. \n",
    "3. Data Processing\n",
    "   - Lack of training and test data:\n",
    "         - Data Preprocessing에서 step 1인 Sentiment Analysis에 따라 filtering 되는 video의 수가 많아서, 학습 및 테스트에 활용할 데이터가 부족하다. 따라서 Video 단위의 filtering 전처리를 제외하고 step 2인 each caption에 대한 emotion assignment 작업만 진행하여 기존의 전체 데이터를 활용하도록 한다.\n",
    "\n",
    "\n",
    "<reference>\n",
    "(1) X. Zhang, M. Li, S. Lin, H. Xu and G. Xiao, \"Transformer-based Multimodal Emotional Perception for Dynamic Facial Expression Recognition in the Wild,\" in IEEE Transactions on Circuits and Systems for Video Technology, doi: 10.1109/TCSVT.2023.3312858.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0880c10-4b3b-4198-956d-6130826210fa",
   "metadata": {},
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be5dad-2970-49aa-b903-a9a4afff1dc1",
   "metadata": {},
   "source": [
    "- 모델 architecture의 변화로 인한 embedding space의 변화를 t-SNE와 같은 visualization approach를 사용하여 시각화한다. \n",
    "- 우리 모델의 목적은 처음보는 text에 대해서 가장 그럴듯한 상위 몇 개의 비디오를 추천하는 모델을 만드는 것이므로, 기존 테스트 데이터셋에도 없는 새로운 \n",
    "    특정 감정에 대한 text (ex) 화난 사람의 비디오) 가 들어갔을 때, 감정이 포함되어있다고 판단되는 비디오의 랭킹 변화를 추적한다.\n",
    "- pipeline / application\n",
    "      - 현재 모델은 v2t, t2v는 recall과 같은 metric만 나오기 때문에, 최종적으로는 비디오를 추천하도록 하는 application 까지 구현해야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ccc67-74ac-41d4-8e64-1be88a9ac9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
