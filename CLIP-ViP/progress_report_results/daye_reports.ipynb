{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60134dd9-ba72-464a-827b-eafe8d6e8589",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35ef2b-c8a1-42a2-8645-8f6f1e5ca6ac",
   "metadata": {},
   "source": [
    "<img src=\"./baseline.png\" alt=\"nn\" width=\"1000\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50015e8-c8a8-40be-a3bd-1eb6ce43061c",
   "metadata": {},
   "source": [
    "# Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbc376-9b3d-48e4-992d-226ada60a46f",
   "metadata": {},
   "source": [
    "<img src=\"./ourEmbeddingModel.png\" alt=\"nn\" width=\"1000\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d7cc7-6358-4c3c-a5a4-25e89873f44a",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "    - Task: video-language alignment (Video-to-Text, Text-to-Video)\n",
    "    - Goal: \n",
    "        - Adapting image-text pre-trained models to video-text pre-training (i.e. post-training) \n",
    "        - Video Proxy mechanism on the basis of CLIP (CLIP-ViP) \n",
    "    - Approach \n",
    "        - In-domain auxiliary data generation: \n",
    "            - to bridge language domain gaps between images and videos datasets \n",
    "            - introduce auxiliary captions into large-scale video-subtitle data to reduce the language domain gap between pre-training and downstream data \n",
    "                - pre-training: Image-Text learning \n",
    "                - downstream data: Video-Text learning \n",
    "\n",
    "        - Video Proxy Mechanism:\n",
    "            - to enable the Vision Transformer (ViT) model for both image and video encoding \n",
    "            - Before feeding into CLIP, we concatenate path tokens with a set of learnable parameters called video proxy tokens\n",
    "            - The output of the first video proxy token will be regarded as the video's representation \n",
    "    - Loss function:\n",
    "        - info-NCE loss \n",
    "\n",
    "## Our Model with emotion embedding \n",
    "    - \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fbd65-33e7-49a9-843f-082caa3798f4",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f198bc-3f50-4eba-968a-947db24cbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7410021-66f0-4b0f-b832-2df37fe742af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\n",
    "\"video\": \"video7112.mp4\",\n",
    "\"video_id\": \"video7112\",\n",
    "\"caption\": \"while other friends too try and hitting the basket another is eager to achieve his fourth successful basket in basketball\",\n",
    "\"duration\": 18.35,\n",
    "\"emotion\": 1,\n",
    "\"positive\": 4.0,\n",
    "\"negative\": 0.0,\n",
    "\"neutral\": 0.0,\n",
    "\"joy\": 4.0,\n",
    "\"trust\": 3.0,\n",
    "\"surprise\": 1.0,\n",
    "\"anticipation\": 3.0,\n",
    "\"fear\": 0.0,\n",
    "\"sadness\": 0.0,\n",
    "\"disgust\": 0.0,\n",
    "\"anger\": 0.0\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb501e29-89af-43ac-8888-0d9abb2f80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "emotion_embedding = nn.Embedding(8, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf737c3-85cb-48cb-b1d4-b1e76f604f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(8, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ddaa3de-04a8-43bb-9047-678829498ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "emotions: tensor([[1., 1., 1., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change non-zero values to 1, effectively binarizing the input\n",
    "emotions = torch.tensor(np.array([4.0, 3.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0])).unsqueeze(0) \n",
    "print(emotions.shape) # (bs, 8)\n",
    "if emotions is not None:\n",
    "    emotions = torch.where(emotions > 0, torch.ones_like(emotions), torch.zeros_like(emotions))\n",
    "    print(f\"emotions: {emotions}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9cc6678-47a9-4d5d-8d76-c8657b802794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all emotion embeddings\n",
    "batch_size = 1 \n",
    "all_emotion_embeds = emotion_embedding.weight.unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size, 8, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78b51f96-b1b6-4be9-a0d2-be1aae53e04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emotion_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b48b6362-506a-4306-a52f-afc1e57cf248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion_mask: tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]), shape = torch.Size([1, 8, 1])\n",
      "\n",
      "selected_motion_embeds: torch.Size([1, 8, 512])\n",
      "\n",
      "emotion_embeds.shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "if emotions is not None:\n",
    "    emotion_mask = emotions.unsqueeze(-1).type_as(all_emotion_embeds)  # [batch_size, 8, 1]\n",
    "    print(f\"emotion_mask: {emotion_mask}, shape = {emotion_mask.shape}\\n\")\n",
    "    selected_emotion_embeds = all_emotion_embeds * emotion_mask  # [batch_size, 8, embed_dim]\n",
    "    print(f\"selected_motion_embeds: {selected_emotion_embeds.shape}\\n\")\n",
    "    emotion_embeds = selected_emotion_embeds.sum(1) / (emotion_mask.sum(1) + 1e-8)  # [batch_size, embed_dim]\n",
    "\n",
    "    print(f\"emotion_embeds.shape: {emotion_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab9731-0ad7-43ef-baaf-d87a796d8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = ? \n",
    "\n",
    "emotion_embeds = emotion_embeds.unsqueeze(1).expand(-1, seq_length, -1)  # [batch_size, seq_length, embed_dim]\n",
    "\n",
    "position_embeddings = self.position_embedding(position_ids)\n",
    "\n",
    "# embedding with additional emotion_embeds \n",
    "embeddings = inputs_embeds + position_embeddings + emotion_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709cb46-fbe4-4621-9b43-174771c783ea",
   "metadata": {},
   "source": [
    "# Future Work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8407c-4d8b-4b33-a6c8-1e4177148b7a",
   "metadata": {},
   "source": [
    "1. Test Data Collection:\n",
    "    - Image-captioning model OFA-Caption (Wang et al., 2022b) to generate one caption for the middle frame of each video in HD-VILA-100M\n",
    "          - with max length of 16 words\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8a779-f8df-4dd2-a413-257ce7c2cc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
