{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28480dd3-e5a2-4964-bd2f-2a1640cfe882",
   "metadata": {},
   "source": [
    "# Emotion Specialized Text to Video Retrieval\n",
    "## Project Progress Report (12 Mar 2024)\n",
    "Team: 14 \n",
    "\n",
    "Members: \n",
    "\n",
    "__Daye Lee__ \n",
    "\n",
    "__Wonseon Lim__ \n",
    "\n",
    "__Hyejin Oh__\n",
    "\n",
    "__Paul Hyunbin Cho__ \n",
    "\n",
    "## Project Objective \n",
    "We aim to create an image text retrieval model that can better find videos taking emotions into account in their queries. In the age of big data, most people now have countless photos and vidoes in their possession, saved. With the immensely large amount of data that they now have, it is becoming increasingly difficult to efficiently find the videos or photos that someone is looking for. In this regard, the field of video-text retrieval has been receiving more attention and their models' capabilities have improved tremendously over the years. However, we believe that performance of these models can be further improved by focusing on the emotional aspects inherent in both the text and the videos. This is especially more important considering growing demand for personalized and emotionally resonant experiences in digital media. __Thus, we hope to develop a tool for users to easily access emotional content in videos, by modifying video-text retrieval models to incorporate emotion data.__ \n",
    "\n",
    "The structure of this project report is as follows:\n",
    "1. Data Collection and Preprocessing\n",
    "2. Model Development\n",
    "3. Experiment Results\n",
    "4. Remaining Project Plan\n",
    "5. Possible Future Directions\n",
    "\n",
    "As the experiments are not run on Jupyter Notebooks, we provide the link to our Github, which contains all of the code used for our experiments and implementations: https://github.com/Daye-Lee18/1517_XPretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e471dc",
   "metadata": {},
   "source": [
    "# 1. Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487593f",
   "metadata": {},
   "source": [
    "\n",
    "- Datasets\n",
    "    - [MSRVTT Data Description]: The MSR-VTT (1) contains 10K YouTube videos with 200K descriptions, each video of which is approximately 10-20 seconds long and consists of 20 captions. We follow our baseline CLIP-ViP (2) to train models on 9K videos, and report results on the 1K test set. \n",
    "    - [Didemo data description]: The DiDeMo (3) consists of 10K Flickr videos annotated with 40K sentences. We follow our baseline CLIP-ViP to evaluate paragraph-to-video retrieval and concatenate all descriptions of a video as one query.\n",
    "    - [MSRVD data description]: The MSVD (4) dataset includes 1,970 videos with approximately 80,000 captions, where train and test are splitted into 1,576 and 394 videos.\n",
    "- Why we use these datasets  \n",
    "    - Our task is to retrieve emotional-specific videos from text, as a downstream task, based on the CLIP-ViP paper. To collect data for our task, we utilized the same data used in the CLIP-ViP paper. Additionally, we can fine-tune on the three datasets mentioned above for downstream task execution with our current computational resources.\n",
    "- Data Preprocessing\n",
    "    - First, we collected three publicly available video captions datasets commonly used to evaluate our base task, text-to-video retrieval. Next, to create a dataset for our main task, emotional text-to-video retrieval, we performed a two-step preprocessing process as follows.\n",
    "    -  Step (1): We performed sentiment analysis on each video caption to determine the presence of sentimental information. The sentiment analysis library calculates neutral, positive, and negative scores for each video caption and decides whether it is positive or negative based on the compound score, which is an overall score derived from these individual scores. For example, if the composite score is above 0.6, we classify the video as positive, and if it is below -0.6, we classify it as negative. Here, 0.6 is the threshold we arbitrarily set to ensure that videos contain robust sentimental information, so it is set higher than 0.5.\n",
    "    $\\rightarrow$ [nltk.sentiment.SentimentIntensityAnalyzer](https://www.nltk.org/api/nltk.sentiment.SentimentIntensityAnalyzer.html?highlight=sentimentintensity)\n",
    "    -  Step (2): After completing the video selection process in the first step, since each video may have multiple captions, we determine the emotional information present in each caption. For this purpose, we used NRCLex, which uses a scoring method based on a predefined lexicon dictionary to calculate eight emotions present in a sentence. We also included the calculation of positive, negative and neutral emotions. Consequently, the resulting data for each caption contains information on eight emotions along with positive, negative, and neutral sentiments.\n",
    "    $\\rightarrow$ [nrclex.NRCLex](https://pypi.org/project/NRCLex/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e59df",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "(1) Jun Xu, Tao Mei, Ting Yao, and Yong Rui. \"Msr-vtt: A large video description dataset for bridging video and language.\" In CVPR, pp. 5288–5296, 2016.\n",
    "\n",
    "(2) Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, Jiebo Luo. \"CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Alignment.\" In ICLR, 2023.\n",
    "\n",
    "(3) Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. \"Localizing moments in video with natural language.\" In ICCV, pp. 5803–5812, 2017.\n",
    "\n",
    "(4) David Chen and William Dolan. \"Collecting highly parallel data for paraphrase evaluation.\" In ACL: Human Language Technologies, pages 190–200, 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ones/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nrclex import NRCLex\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53bf02",
   "metadata": {},
   "source": [
    "## 1.1 MSRVTT\n",
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d76c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 4) (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "mrsvtt_train9k_dir = \"./data/msrvtt/annotations/msrvtt_ret_train9k.json\"\n",
    "mrsvtt_test1k_dir = \"./data/msrvtt/annotations/msrvtt_ret_test1k.json\"\n",
    "\n",
    "with open(mrsvtt_train9k_dir, 'r') as f:\n",
    "    mrsvtt_train9k_json = json.load(f)\n",
    "\n",
    "with open(mrsvtt_test1k_dir, 'r') as f:\n",
    "    mrsvtt_test1k_json = json.load(f)\n",
    "\n",
    "# Extract video_id\n",
    "train9k = pd.DataFrame(mrsvtt_train9k_json)\n",
    "test1k = pd.DataFrame(mrsvtt_test1k_json)\n",
    "train9k['video_id'] = train9k['video'].apply(lambda x: x.split('.')[0])\n",
    "test1k['video_id'] = test1k['video'].apply(lambda x: x.split('.')[0])\n",
    "print(train9k.shape, test1k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ee3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 2) (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "#  Group captions by each video_id\n",
    "merge_caption = lambda x: '. '.join(x)\n",
    "train_video_text = train9k.groupby('video_id').aggregate('caption').apply(merge_caption)\n",
    "test_video_text = test1k.groupby('video_id').aggregate('caption').apply(merge_caption)\n",
    "train_video_text = pd.DataFrame(train_video_text).reset_index()\n",
    "test_video_text = pd.DataFrame(test_video_text).reset_index()\n",
    "print(train_video_text.shape, test_video_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c50aff",
   "metadata": {},
   "source": [
    "### Step (1): Extract Sentimental Video using Sentiment Analysis\n",
    "\n",
    "We used two methods to extract videos with emotional information: one using SentimentIntensityAnalyzer() and the other involving manual extraction of videos with well-defined emotional information. The extract_emotional_data function uses SentimentIntensityAnalyzer(), while the extract_emotion_manually function uses manual extraction. Out of 7,010 videos in the MSRVTT dataset, 6,137 videos in the training set contain emotional information, and out of 1,000 videos in the test set, 34 videos contain emotional information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feae60e",
   "metadata": {},
   "source": [
    "### Functions Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion_manually(df, text=None):\n",
    "    # assign the default text\n",
    "    if text is None:\n",
    "        text = \"happy|sad|afraid|fear|surprise|joy|disgust|annoy| anger|angry|\" \\\n",
    "                \"excite|excited|exciting|scare|scared|scary|fright|frighten|frightened|frightening\" \\\n",
    "                \"|fearful|fearless|fearfully\"\n",
    "    # extract the data that contains the text\n",
    "    manual_df = df[df['caption'].str.contains(text)]\n",
    "    print(\"Number of manually selecting data:\", len(manual_df))\n",
    "    return manual_df\n",
    "\n",
    "\n",
    "def extract_emotional_data(df, sent_bound=0.6, manual=False):\n",
    "    # Initialize SentimentIntensityAnalyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    emotion_df = pd.DataFrame()\n",
    "\n",
    "    # extract the emotional data using the sentiment analysis\n",
    "    for idx in tqdm(range(len(df))):\n",
    "        sentence = df.caption[idx]\n",
    "        # calculate the sentiment score\n",
    "        sentiment_score = sia.polarity_scores(sentence)\n",
    "        # determine the emotional data: compound score > 0.6 (positive) or < -0.6 (negative)\n",
    "        if sentiment_score['compound'] > sent_bound or sentiment_score['compound'] < -sent_bound:\n",
    "            emotion_df = pd.concat([emotion_df, df.iloc[idx:idx+1, :]])\n",
    "\n",
    "    print(\"Number of emotional data:\", len(emotion_df))\n",
    "\n",
    "    # extract the data that contains predefined emotion words\n",
    "    if manual:\n",
    "        manual_df  = extract_emotion_manually(df)\n",
    "        # extract the data that is not in the emotion_df\n",
    "        if \"sen_id\" in manual_df.columns:\n",
    "            only_manual_df = pd.merge(manual_df, emotion_df, on='sen_id', how='outer', indicator=True).query('_merge==\"left_only\"')\n",
    "            only_manual_df = only_manual_df.drop(columns=[\"caption_y\", \"video_id_y\", \"_merge\"]).rename(columns={\"caption_x\": \"caption\", \"video_id_x\": \"video_id\"})\n",
    "        else:\n",
    "            only_manual_df = pd.merge(manual_df, emotion_df, on='video_id', how='outer', indicator=True).query('_merge==\"left_only\"')\n",
    "            only_manual_df = only_manual_df.drop(columns=[\"caption_y\", \"_merge\"]).rename(columns={\"caption_x\": \"caption\"})\n",
    "        print(\"Number of data only in manual data:\", len(only_manual_df))\n",
    "        emotion_df = pd.concat([emotion_df, only_manual_df])\n",
    "        print(\"Toal number of emotional data:\", len(emotion_df))\n",
    "        return emotion_df\n",
    "\n",
    "    return emotion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe4804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9000/9000 [00:17<00:00, 526.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emotional data: 5984\n",
      "Number of manually selecting data: 1019\n",
      "Number of data only in manual data: 153\n",
      "Toal number of emotional data: 6137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 5879.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emotional data: 29\n",
      "Number of manually selecting data: 7\n",
      "Number of data only in manual data: 5\n",
      "Toal number of emotional data: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract videos that contain emotional\n",
    "emotion_train_row_df = extract_emotional_data(train_video_text, sent_bound=0.6, manual=True)\n",
    "emotion_test_row_df = extract_emotional_data(test_video_text, sent_bound=0.6, manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3af879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (122740, 5) (2863, 3)\n",
      "Test shape: (34, 3) (34, 3)\n",
      "Total Test shape: (34, 3) (966, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/40rtdpp937lfzqpr346g_0bh0000gn/T/ipykernel_9346/3657727822.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  no_emotion_train_row_df.loc[:, 'emotion'] = 0\n",
      "/var/folders/k0/40rtdpp937lfzqpr346g_0bh0000gn/T/ipykernel_9346/3657727822.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  emotion_train_row_df.loc[:, 'emotion'] = 1\n",
      "/var/folders/k0/40rtdpp937lfzqpr346g_0bh0000gn/T/ipykernel_9346/3657727822.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  no_emotion_test_row_df['emotion'] = 0\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "no_emotion_train_row_df = train_video_text[~train_video_text.video_id.isin(emotion_train_row_df.video_id)]\n",
    "no_emotion_train_row_df.loc[:, 'emotion'] = 0\n",
    "emotion_train_row_df.loc[:, 'emotion'] = 1\n",
    "print('Train shape:', emotion_train_row_df.shape, no_emotion_train_row_df.shape)\n",
    "train_df = pd.concat([emotion_train_row_df, no_emotion_train_row_df])\n",
    "\n",
    "# test set\n",
    "num_emo_test = emotion_test_row_df.shape[0]\n",
    "no_emotion_test_row_df = test_video_text[~test_video_text.video_id.isin(emotion_test_row_df.video_id)].sample(num_emo_test)\n",
    "no_emotion_test_row_df['emotion'] = 0\n",
    "emotion_test_row_df['emotion'] = 1\n",
    "print('Test shape:', emotion_test_row_df.shape, no_emotion_test_row_df.shape)\n",
    "test_row_df = pd.concat([emotion_test_row_df, no_emotion_test_row_df])\n",
    "# total test set\n",
    "no_emotion_test_row_df = test_video_text[~test_video_text.video_id.isin(emotion_test_row_df.video_id)]\n",
    "no_emotion_test_row_df['emotion'] = 0\n",
    "emotion_test_row_df['emotion'] = 1\n",
    "print('Total Test shape:', emotion_test_row_df.shape, no_emotion_test_row_df.shape)\n",
    "total_test_row_df = pd.concat([emotion_test_row_df, no_emotion_test_row_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2086156c",
   "metadata": {},
   "source": [
    "### Check the sentimental compound score\n",
    "\n",
    "To check the sentimental value of the data, we evaluate the sentimental test data extracted using the compound score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a7765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while other friends too try and hitting the basket another is eager to achieve his fourth successful basket in basketball\n",
      "{'neg': 0.0, 'neu': 0.644, 'pos': 0.356, 'compound': 0.8555}\n",
      "\n",
      "a young girl in a horror movie is haunted\n",
      "{'neg': 0.576, 'neu': 0.424, 'pos': 0.0, 'compound': -0.7783}\n",
      "\n",
      "a female soccer player accepts a reward while being cheered on by the crowd\n",
      "{'neg': 0.0, 'neu': 0.492, 'pos': 0.508, 'compound': 0.8519}\n",
      "\n",
      "a woman giving skin care tips\n",
      "{'neg': 0.0, 'neu': 0.349, 'pos': 0.651, 'compound': 0.6808}\n",
      "\n",
      "vladmir putin talks on the news about the fight against terrorism\n",
      "{'neg': 0.444, 'neu': 0.556, 'pos': 0.0, 'compound': -0.802}\n",
      "\n",
      "jolly good music troop delivering a program and the lady is in good spirit\n",
      "{'neg': 0.0, 'neu': 0.455, 'pos': 0.545, 'compound': 0.8689}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentiment score\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "count = 0\n",
    "for text in total_test_row_df.caption:\n",
    "    if count > 5:\n",
    "        break\n",
    "    print(text)\n",
    "    print(sia.polarity_scores(text))\n",
    "    print()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098da00",
   "metadata": {},
   "source": [
    "### Step (2): Assign Specific Emotion\n",
    "\n",
    "Using the lexicon provided by the NRC Lexicon Library, we assigned emotion scores to each caption, allowing us to determine positive, negative, and neutral sentiments. In addition, we performed scoring for the eight emotion types defined by Robert Plutchik: joy, trust, fear, surprise, sadness, disgust, anger, and anticipation. This results in a total of 11 columns in the original dataset.\n",
    "\n",
    "We applied this process to three datasets in the second step of assigning emotional information: the refined 6K training data from the first step, a combination of 34 sentimental and 34 non-sentimental data for a total of 68 test data, and the entire 1K test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions that NRC Lexicon library has\n",
    "emostion_list = [\n",
    "    \"joy\",\n",
    "    \"trust\",\n",
    "    \"fear\",\n",
    "    \"surprise\",\n",
    "    \"sadness\",\n",
    "    \"disgust\",\n",
    "    \"anger\",\n",
    "    \"anticipation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b1c99",
   "metadata": {},
   "source": [
    "### Functions Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_columns(df):\n",
    "    \"\"\" Create sentiment columns using NRC lexicon\n",
    "        : positive, negative, neutral\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    for idx in tqdm(range(df.shape[0])):\n",
    "        emotion_counts = NRCLex(df.caption.iloc[idx]).raw_emotion_scores\n",
    "        # positive case\n",
    "        if 'positive' in emotion_counts.keys():\n",
    "            df.loc[idx, 'positive'] = emotion_counts['positive']\n",
    "        # negative case\n",
    "        if 'negative' in emotion_counts.keys():\n",
    "            df.loc[idx, 'negative'] = emotion_counts['negative']\n",
    "        # neutral case\n",
    "        if 'positive' not in emotion_counts.keys() and 'negative' not in emotion_counts.keys():\n",
    "            df.loc[idx, 'neutral'] = 1\n",
    "\n",
    "    df.fillna({'positive': 0, 'negative': 0, 'neutral': 0}, inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_emotion_columns(df):\n",
    "    \"\"\" Create emotion columns using NRC lexicon\n",
    "        : anger, anticipation, disgust, fear, joy, sadness, surprise, trust\n",
    "    \"\"\"\n",
    "    df = df.reset_index(drop=True)\n",
    "    for idx in tqdm(range(df.shape[0])):\n",
    "        emotion_counts = NRCLex(df.caption.iloc[idx]).raw_emotion_scores\n",
    "        # create the emotion columns\n",
    "        for emo in emostion_list:\n",
    "            if emo in emotion_counts.keys():\n",
    "                df.loc[idx, emo] = emotion_counts[emo]\n",
    "\n",
    "    df.fillna({emo: 0 for emo in emostion_list}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/122740 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122740/122740 [01:20<00:00, 1533.17it/s]\n",
      "100%|██████████| 122740/122740 [01:04<00:00, 1895.97it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 1865.87it/s]\n",
      "100%|██████████| 68/68 [00:00<00:00, 1592.66it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2152.19it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2311.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# create the sentiment columns\n",
    "emotion_train_row_df = create_sentiment_columns(emotion_train_row_df)\n",
    "# create the emotion columns\n",
    "emotion_train_row_df = create_emotion_columns(emotion_train_row_df)\n",
    "\n",
    "# Test\n",
    "# create the sentiment columns\n",
    "test_row_df = create_sentiment_columns(test_row_df)\n",
    "# create the emotion columns\n",
    "test_row_df = create_emotion_columns(test_row_df)\n",
    "\n",
    "# Total Test\n",
    "# create the sentiment columns\n",
    "total_test_row_df = create_sentiment_columns(total_test_row_df)\n",
    "# create the emotion columns\n",
    "total_test_row_df = create_emotion_columns(total_test_row_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9471716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "emotion_train_row_df.to_json('msrvtt_train6k.json', orient='records', lines=False)\n",
    "# test\n",
    "test_row_df.to_json('msrvtt_test68.json', orient='records', lines=False)\n",
    "# total test\n",
    "total_test_row_df.to_json('msrvtt_test1k.json', orient='records', lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3be56",
   "metadata": {},
   "source": [
    "## 1.2 DiDemo\n",
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113909b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37182, 3) (4017, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "train_dir = \"./data/didemo/annotations/didemo_retrieval_train.json\"\n",
    "val_dir = \"./data/didemo/annotations/didemo_retrieval_val.json\"\n",
    "test_dir = \"./data/didemo/annotations/didemo_retrieval_test.json\"\n",
    "\n",
    "with open(train_dir, \"r\") as f:\n",
    "    train_json = json.load(f)\n",
    "\n",
    "with open(val_dir, \"r\") as f:\n",
    "    val_json = json.load(f)\n",
    "\n",
    "with open(test_dir, \"r\") as f:\n",
    "    test_json = json.load(f)\n",
    "\n",
    "# train\n",
    "train_dict = {'video': [], 'caption': []}\n",
    "for i, data in enumerate(train_json + val_json):\n",
    "    cap_len = len(data['caption_list'])\n",
    "    for j in range(cap_len):\n",
    "        train_dict['video'].append(data['video'])\n",
    "        train_dict['caption'].append(data['caption_list'][j])\n",
    "\n",
    "# test\n",
    "test_dict = {'video': [], 'caption': []}\n",
    "for i, data in enumerate(test_json):\n",
    "    cap_len = len(data['caption_list'])\n",
    "    for j in range(cap_len):\n",
    "        test_dict['video'].append(data['video'])\n",
    "        test_dict['caption'].append(data['caption_list'][j])\n",
    "\n",
    "# Extract video_id\n",
    "train9k = pd.DataFrame(train_dict)\n",
    "test1k = pd.DataFrame(test_dict)\n",
    "train9k['video_id'] = train9k['video'].apply(lambda x: x.split('.')[0])\n",
    "test1k['video_id'] = test1k['video'].apply(lambda x: x.split('.')[0])\n",
    "print(train9k.shape, test1k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c944a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9459, 2) (1003, 2)\n"
     ]
    }
   ],
   "source": [
    "#  Group captions by each video_id\n",
    "merge_caption = lambda x: '. '.join(x)\n",
    "train_video_text = train9k.groupby('video_id').aggregate('caption').apply(merge_caption)\n",
    "test_video_text = test1k.groupby('video_id').aggregate('caption').apply(merge_caption)\n",
    "train_video_text = pd.DataFrame(train_video_text).reset_index()\n",
    "test_video_text = pd.DataFrame(test_video_text).reset_index()\n",
    "print(train_video_text.shape, test_video_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7c0ba",
   "metadata": {},
   "source": [
    "### Step (1): Extract Sentimental Video using Sentiment Analysis\n",
    "\n",
    "Out of 9,459 videos in the DiDemo dataset, 668 videos in the training set contain sentimental information, and out of 1,003 videos in the test set, 77 videos contain sentimental information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa8892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9459/9459 [00:03<00:00, 2390.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emotional data: 627\n",
      "Number of manually selecting data: 59\n",
      "Number of data only in manual data: 41\n",
      "Toal number of emotional data: 668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1003/1003 [00:00<00:00, 2329.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emotional data: 75\n",
      "Number of manually selecting data: 7\n",
      "Number of data only in manual data: 2\n",
      "Toal number of emotional data: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract videos that contain emotional\n",
    "emotion_train_row_df = extract_emotional_data(train_video_text, sent_bound=0.6, manual=True)\n",
    "emotion_test_row_df = extract_emotional_data(test_video_text, sent_bound=0.6, manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057161e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (668, 3) (8791, 3)\n",
      "Test shape: (77, 3) (77, 3)\n",
      "Total Test shape: (77, 3) (926, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/40rtdpp937lfzqpr346g_0bh0000gn/T/ipykernel_9346/3657727822.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  no_emotion_train_row_df.loc[:, 'emotion'] = 0\n",
      "/var/folders/k0/40rtdpp937lfzqpr346g_0bh0000gn/T/ipykernel_9346/3657727822.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  no_emotion_test_row_df['emotion'] = 0\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "no_emotion_train_row_df = train_video_text[~train_video_text.video_id.isin(emotion_train_row_df.video_id)]\n",
    "no_emotion_train_row_df.loc[:, 'emotion'] = 0\n",
    "emotion_train_row_df.loc[:, 'emotion'] = 1\n",
    "print('Train shape:', emotion_train_row_df.shape, no_emotion_train_row_df.shape)\n",
    "train_df = pd.concat([emotion_train_row_df, no_emotion_train_row_df])\n",
    "\n",
    "# test set\n",
    "num_emo_test = emotion_test_row_df.shape[0]\n",
    "no_emotion_test_row_df = test_video_text[~test_video_text.video_id.isin(emotion_test_row_df.video_id)].sample(num_emo_test)\n",
    "no_emotion_test_row_df['emotion'] = 0\n",
    "emotion_test_row_df['emotion'] = 1\n",
    "print('Test shape:', emotion_test_row_df.shape, no_emotion_test_row_df.shape)\n",
    "test_row_df = pd.concat([emotion_test_row_df, no_emotion_test_row_df])\n",
    "# total test set\n",
    "no_emotion_test_row_df = test_video_text[~test_video_text.video_id.isin(emotion_test_row_df.video_id)]\n",
    "no_emotion_test_row_df['emotion'] = 0\n",
    "emotion_test_row_df['emotion'] = 1\n",
    "print('Total Test shape:', emotion_test_row_df.shape, no_emotion_test_row_df.shape)\n",
    "total_test_row_df = pd.concat([emotion_test_row_df, no_emotion_test_row_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d90a18",
   "metadata": {},
   "source": [
    "### Check the sentimental compound score\n",
    "\n",
    "To check the sentimental value of the data, we evaluate the sentimental test data extracted using the compound score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cbb03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man starts talking in microphone. words apear on bottom first time. the gentleman puts his left arm underneath his right arm.. man talking on microphone unfolds arm and puts hand behind his head. a man places his hand on the back of his head.\n",
      "{'neg': 0.0, 'neu': 0.865, 'pos': 0.135, 'compound': 0.7506}\n",
      "\n",
      "silohette of a woman holding a child can be seen in the fog. the screen goes blue and we see no people.. darkest blue. no silhouettes are visible\n",
      "{'neg': 0.248, 'neu': 0.752, 'pos': 0.0, 'compound': -0.765}\n",
      "\n",
      "bright flash goes off like someone took a picture\n",
      "{'neg': 0.0, 'neu': 0.526, 'pos': 0.474, 'compound': 0.6597}\n",
      "\n",
      "a hand first appears. man playing a touchscreen game.. the circle on the screen is now yellow.. the yellow button turns green and is pressed. the men are filmed.\n",
      "{'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'compound': 0.6124}\n",
      "\n",
      "people watching a blue screen and laughing. we see the lady on the left hold up one finger. women touching screen holds up her left index finger. man with his hands in his back pockets comes into view. we are finished looking at the people playing on the screen\n",
      "{'neg': 0.0, 'neu': 0.902, 'pos': 0.098, 'compound': 0.6124}\n",
      "\n",
      "the person's right hand first comes into view of the camera.. man spins the fan up for the first time. the blades on helicopter are spinning. contraption spins in circles for the first time. contraption falls off the table and man puts it back up\n",
      "{'neg': 0.0, 'neu': 0.887, 'pos': 0.113, 'compound': 0.6705}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentiment score\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "count = 0\n",
    "for text in total_test_row_df.caption:\n",
    "    if count > 5:\n",
    "        break\n",
    "    print(text)\n",
    "    print(sia.polarity_scores(text))\n",
    "    print()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51acce2f",
   "metadata": {},
   "source": [
    "### Step (2): Assign Specific Emotion\n",
    "\n",
    "We applied this process to three datasets: 668 training data refined in step 1, 154 test data combining 77 sentimental and 77 non-sentimental data, and the entire 1K test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19aca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/668 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 668/668 [00:01<00:00, 474.11it/s]\n",
      "100%|██████████| 668/668 [00:00<00:00, 775.48it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 926.33it/s]\n",
      "100%|██████████| 154/154 [00:00<00:00, 817.62it/s]\n",
      "100%|██████████| 1003/1003 [00:01<00:00, 949.50it/s]\n",
      "100%|██████████| 1003/1003 [00:01<00:00, 865.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# create the sentiment columns\n",
    "emotion_train_row_df = create_sentiment_columns(emotion_train_row_df)\n",
    "# create the emotion columns\n",
    "emotion_train_row_df = create_emotion_columns(emotion_train_row_df)\n",
    "\n",
    "# Test\n",
    "# create the sentiment columns\n",
    "test_row_df = create_sentiment_columns(test_row_df)\n",
    "# create the emotion columns\n",
    "test_row_df = create_emotion_columns(test_row_df)\n",
    "\n",
    "# Total Test\n",
    "# create the sentiment columns\n",
    "total_test_row_df = create_sentiment_columns(total_test_row_df)\n",
    "# create the emotion columns\n",
    "total_test_row_df = create_emotion_columns(total_test_row_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "emotion_train_row_df.to_json('didemo_train668.json', orient='records', lines=False)\n",
    "# test\n",
    "test_row_df.to_json('didemo_test154.json', orient='records', lines=False)\n",
    "# total test\n",
    "total_test_row_df.to_json('didemo_test1k.json', orient='records', lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f4e17",
   "metadata": {},
   "source": [
    "## 1.3 MSVD\n",
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_each_caption(df):\n",
    "    data_dict = {}\n",
    "    data_dict['caption'] = []\n",
    "    data_dict['video_id'] = []\n",
    "    for id, cap in df.values:\n",
    "        cap_len = len(cap.split('.'))\n",
    "        for i in range(cap_len):\n",
    "            data_dict['caption'].append(cap.split('.')[i])\n",
    "            data_dict['video_id'].append(id)\n",
    "    return pd.DataFrame(data_dict)[['video_id', 'caption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad06e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80827/80827 [00:00<00:00, 296552.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total video:  1970\n",
      "(1576, 2) (394, 2)\n",
      "(37182, 3) (4017, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "msvd_dir = \"./data/msvd/annotations/AllVideoDescriptions.txt\"\n",
    "\n",
    "with open(msvd_dir, 'r') as f:\n",
    "    msvd_text = f.readlines()\n",
    "msvd_text = msvd_text[7:]\n",
    "# parse msvd_test to dictionary type\n",
    "msvd_dict = {}\n",
    "for line in tqdm(msvd_text):\n",
    "    line = line.split()\n",
    "    video_id = line[0]\n",
    "    description = \" \".join(line[1:])\n",
    "    if video_id in msvd_dict.keys():\n",
    "        msvd_dict[video_id] += \". \" + description\n",
    "    else:\n",
    "        msvd_dict[video_id] = description\n",
    "\n",
    "video_text = pd.DataFrame(msvd_dict.items(), columns=['video_id', 'caption'])\n",
    "print(\"The number of total video: \", video_text.video_id.nunique())\n",
    "train_video_text, test_video_text = train_test_split(video_text, test_size=0.2, random_state=42)\n",
    "train_video_text, test_video_text = train_video_text.reset_index(drop=True), test_video_text.reset_index(drop=True)\n",
    "print(train_video_text.shape, test_video_text.shape)\n",
    "\n",
    "# train & test\n",
    "train1k = split_each_caption(train_video_text)\n",
    "test400 = split_each_caption(test_video_text)\n",
    "print(train9k.shape, test1k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a43fa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1576 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1576/1576 [00:03<00:00, 482.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emotional data: 1060\n",
      "Number of manually selecting data: 186\n",
      "Number of data only in manual data: 32\n",
      "Toal number of emotional data: 1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394/394 [00:00<00:00, 717.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emotional data: 254\n",
      "Number of manually selecting data: 37\n",
      "Number of data only in manual data: 3\n",
      "Toal number of emotional data: 257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# extract videos that contain emotional\n",
    "emotion_train_row_df = extract_emotional_data(train_video_text, sent_bound=0.6, manual=True)\n",
    "emotion_test_row_df = extract_emotional_data(test_video_text, sent_bound=0.6, manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25451f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1092, 3) (484, 3)\n",
      "Test shape: (257, 3) (137, 3)\n",
      "Total Test shape: (257, 3) (137, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k0/40rtdpp937lfzqpr346g_0bh0000gn/T/ipykernel_9346/146166510.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  no_emotion_train_row_df.loc[:, 'emotion'] = 0\n",
      "/var/folders/k0/40rtdpp937lfzqpr346g_0bh0000gn/T/ipykernel_9346/146166510.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  no_emotion_test_row_df['emotion'] = 0\n"
     ]
    }
   ],
   "source": [
    "# training set\n",
    "no_emotion_train_row_df = train_video_text[~train_video_text.video_id.isin(emotion_train_row_df.video_id)]\n",
    "no_emotion_train_row_df.loc[:, 'emotion'] = 0\n",
    "emotion_train_row_df.loc[:, 'emotion'] = 1\n",
    "print('Train shape:', emotion_train_row_df.shape, no_emotion_train_row_df.shape)\n",
    "train_df = pd.concat([emotion_train_row_df, no_emotion_train_row_df])\n",
    "\n",
    "# test set\n",
    "num_emo_test = min(emotion_test_row_df.shape[0], test_video_text.shape[0] - emotion_test_row_df.shape[0])\n",
    "no_emotion_test_row_df = test_video_text[~test_video_text.video_id.isin(emotion_test_row_df.video_id)].sample(num_emo_test)\n",
    "no_emotion_test_row_df['emotion'] = 0\n",
    "emotion_test_row_df['emotion'] = 1\n",
    "print('Test shape:', emotion_test_row_df.shape, no_emotion_test_row_df.shape)\n",
    "test_row_df = pd.concat([emotion_test_row_df, no_emotion_test_row_df])\n",
    "# total test set\n",
    "no_emotion_test_row_df = test_video_text[~test_video_text.video_id.isin(emotion_test_row_df.video_id)]\n",
    "no_emotion_test_row_df['emotion'] = 0\n",
    "emotion_test_row_df['emotion'] = 1\n",
    "print('Total Test shape:', emotion_test_row_df.shape, no_emotion_test_row_df.shape)\n",
    "total_test_row_df = pd.concat([emotion_test_row_df, no_emotion_test_row_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b198c8b",
   "metadata": {},
   "source": [
    "### Check the sentimental compound score\n",
    "\n",
    "To check the sentimental value of the data, we evaluate the sentimental test data extracted using the compound score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d5947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man. a man is playing a guitar. a man is playing guitar on stage. john denver is on stage playing a guitar and singing. a man is playing and singing. a man is singing and playing guitar. john denver is singing and playing a guitar on stage. a man is singing on stage. a man is playing a guitar and singing on a stage. a man is playing a guitar. a man is playing guitar. someone is singing. john denver is playing a guitar. a man standing on stage is singing a song while strumming a guitar along with a band of musicians. john denver performed with his band. the man is singing and playing the guitar. john denver is playing his guitar and singing. a man plays the guitar. the man sang country songs on stage. the men is singing. a band is playing on stage. a man is playing guiatar. john denver singing and playing a guitar. a man is playing a guitar on stage. john denver is singing song with guiter. john denver sang and played his guitar on stage. a man singing a song. the man is singing and playing the guitar. a man is singing and playing guitar\n",
      "{'neg': 0.0, 'neu': 0.808, 'pos': 0.192, 'compound': 0.9744}\n",
      "\n",
      "a dog is staying rolled over. a dog is laying on his back. a dog is laying on its back. a dog is laying on the floor. a dog is lying on its back on the floor. a dog is lying on its back. a dog is lying on the floor on its back. a dog is rolling over the floor. a dog is sleeping on a floor. a dog lies on his back on a wooden floor. a dog lies on its back. a woman is instructing a white dog which is lying on its back on the floor. the dog is lying down on the floor. the dog is lying on his back. a white dog with a black patch on one eye. a dog is lading on a room ground. a dog is lying in the floor. a jack russell dog named patchy who wo nt listen to anything his owner tells him. a jack russell is lying on the ground. a pyppy is playing. dogs wake up while sleeping. a silly dog putting his legs up. a bitch is lying on her back on the floor. a dog is laying. a dog is laying on her back. a dog is laying on his back. a dog is laying on his back. a dog is laying on it s back. a dog is laying on the floor. a dog is laying on the ground. a dog is lying on her back. a dog is lying on her back. a dog is lying on its back. a dog is lying on its back. a dog is lying on the floor. a dog is lying on the floor. a dog is lying on the floor. a dog lays on the floor. a dog lies on it s back. a dog lies on the floor with its tummy facing up. a dog on it s back. a dogs are playing. a female terrier lies bellyup. a jack russell terrier lays on its back. a lactating dog is lying on the floor. the dog is lying down. the dog is lying on her back. the dog lay on his back so that someone would scratch his stomach. the dog lay on his back waiting for his tummy to be rubbed. a dog is doing silly things\n",
      "{'neg': 0.21, 'neu': 0.776, 'pos': 0.015, 'compound': -0.9976}\n",
      "\n",
      "someone is playing the piano. a person plays a piano. a person is playing a piano. a person is playing a keyboard. a person is playing a song on a piano. someone is playing a piano. a man is playing piano. a person is playing the piano. someone is playing a piano. someone is playing the piano. someone is playing paino. a person is playing the piano. the boy played the piano. the person is playing the piano. someone is playing on a piano. a man plays the piano. the man played the piano. the boy is playing the key borad. a man is playing the piano. a man playing the piono. anyone is playing instrument. a boy is playing the piano. piano player. playing the piano. a person is playing a piano. someone is playing a piano. a child is playing the piano. the boy sat at the piano and played. a baby playing piano. the boy is playing the piano. someone plays piano. a man is playing a keyboard. playing a piano keyboard\n",
      "{'neg': 0.0, 'neu': 0.659, 'pos': 0.341, 'compound': 0.9906}\n",
      "\n",
      "baby otters playing in a pool. a group of baby otters are playing in the water. a group of otters are playing in the water. a group of otters are playing in water. a group of otters plays in the water. animals are playing in water. baby otters are playing in groups in muddy water. baby otters are playing in the water. baby otters are playing in the water. baby otters are playing in water. otters are playing in the water. otters are playing in water. otters play at the edge of the water. several baby otters are playing in the water. several otters are playing in the water. some otters are swimming. the otters are playing in the water. the se otters playyed in the water. the sea otters played together in the water. a bunch of baby otters are playing in the water. a small dolphins swimming on the beach. baby otters are fighting each other. baby otters are playing in a pool. baby otters play in the water. baby otters playing with each other in water. lots of otter are playing in water. some baby otters playing. a baby otters. a group of baby otters are playing in the water. a group of baby otters are playing together. animals are bathing in water. baby ottar is swimming. baby otters are playing in the water. baby otters are playing. baby otters are playing. baby otters are swimming and playing. baby otters playing in water. baby otters were swimming in water. cute baby otters. group of baby otters playing in water. otters are fighting. otters are moving in water. some baby otters are playing in the water. some baby otters playing in water. some otters playing in water. baby otters at sea world are in their pool playing together. these are baby otters\n",
      "{'neg': 0.015, 'neu': 0.777, 'pos': 0.207, 'compound': 0.9912}\n",
      "\n",
      "a man is slicing a row of mushrooms very quickly. a man is shown chopping several potatoes into small pieces using a knife. a man is cutting up mushrooms very quickly. mushrooms are being chopped. a man is slicing potato. a person is quickly chopping mushrooms. a person is quickly chopping mushrooms with a knife. a man dices several mushrooms. a man is cutting mushrooms. a man is rapidly chopping some mushrooms using a knife. a person chopping potatoes very fast. a man chopped up potatoes. the person is slicing mushrooms very quickly. a row of mushrooms is cut into slices. a man is chopping mushrooms very quickly. a man chopped potatoes. a man is slicing mashroom. a guy is slicing the mushroom. a man is cutting mushrooms. a man is slicing up some mushrooms. a men slicing mushrooms speed test. a person is chopping mushrooms. man chopping with amazing speed. the chef is slicing the potato. the person cut the vegetables very fast. a chef is slicing up some mushrooms. a man chops very fast. a man cutting mushrooms with a knife in a very high speed. a man is chopping up a line of food in a kitchen. a man is cutting a mushroom. a man is cutting mushrooms into pieces. a man is cutting up mushrooms rather quickly. a man is slicing mushrooms at high speed. a man is slicing mushrooms. a man is slicing mushrooms. a man is slicing mushrooms. a man is slicing potato. a man is slicing some vegetables. a man is speedily slicing large mushrooms with a large knife. a man quickly chops mushrooms. a man sis slicing the mushrooms. a man slicing mushrooms. a man slicing mushrooms. a person cutting onions. a person is chopping a row of potatoes. a person is cutting mushrooms very fast. a person is quickly slicing mushrooms with a knife. a slicing of mushrooms. chopping potatoes with a knife. r slicing mushrooms speed test 1. someone chops mushrooms. someone cuts mushrooms quickly. someone is chopping mushrooms on a cutting board. someone is chopping mushrooms very quickly. someone is chopping potatoes. someone is slicing a potato. someone is slicing mushrooms. the person is cutting the aloe. a man is slicing mushroom with a knife. the person is cutting something. a man do cutting potato with sharp skills. how cut food\n",
      "{'neg': 0.081, 'neu': 0.908, 'pos': 0.011, 'compound': -0.9042}\n",
      "\n",
      "a cat get his head stuck in a bag. a cat gets his head stuck in a paper bag. a cat gets its head stuck in a small paper bag. a cat is playing with a paper bag and gets it stuck on its head. a cat is walking with a bag on its head. a cat is wearing a bag on its head. a cat puts a cover on his head. a cat puts its head in a paper bag then manages to remove the paper bag from its head. a cat putting it s head in a bag. a cat sticks its head into a paper bag and walks around. a cat walks across a couch. a cat s head is stuck in a paper bag. a grey cat walking on a sofa pushes his head into a brown cover and finally manages to get it off after struggling for a while. the cat had his head in a paper bag. the cat had his head in a paper bag. the cat has a paper bag on it s head. a strange cat put his face in the paper pocket. a cat s head is stuck inside a paper bag. a cat puts a bag on his head and then takes it back off. cat playing with a bag. a cat is entering its head into a cover. the cat is leaving the head inside the cover. a cat puts his head in a bag. a cat put its head in to the pack. a cat is moving with a packet on the face. a cat is playing on the sofa. a strange cat is covering its face. a cat is sarching into the cover. a cat gets a bag stuck on its head. the cat maru is covering its face with a cover and a few minutes later it had taken that. a cat is covering his mouth with a envelope. a cat sticks its head in a bag. a cat sticks his head into a paper bag. a cat is putting its head into a brown paper cover. a cat trying to remove the face cover. a cat is playing on sofa. a cat has a bag on its head. a cat walking on a sofa and trying to grab something from a packet. a cat doing some kind of strange tricks on sofa. a cat put its head in a paper bag and gets it stuck. the cat is playing with a cover. a cat is playing. cats funny comedy. the strangest cat on the planet. a cat pushes a paper bag over its head and subsequently has trouble removing it. a cat is trying to free herself. a cat is slipping its head into a paper bag. a cat gets a paper bag stuck on its head\n",
      "{'neg': 0.073, 'neu': 0.877, 'pos': 0.05, 'compound': -0.7717}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sentiment score\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "count = 0\n",
    "for text in total_test_row_df.caption:\n",
    "    if count > 5:\n",
    "        break\n",
    "    print(text)\n",
    "    print(sia.polarity_scores(text))\n",
    "    print()\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bac366",
   "metadata": {},
   "source": [
    "### Step (2): Assign Specific Emotion\n",
    "\n",
    "We applied this process to three datasets: 1,092 training data refined in step 1, and the entire 1K test data, chosen for its balance of sentimental and non-sentimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e95eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1092 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1092/1092 [00:09<00:00, 119.37it/s]\n",
      "100%|██████████| 1092/1092 [00:09<00:00, 110.41it/s]\n",
      "100%|██████████| 394/394 [00:02<00:00, 135.39it/s]\n",
      "100%|██████████| 394/394 [00:03<00:00, 118.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "# create the sentiment columns\n",
    "emotion_train_row_df = create_sentiment_columns(emotion_train_row_df)\n",
    "# create the emotion columns\n",
    "emotion_train_row_df = create_emotion_columns(emotion_train_row_df)\n",
    "\n",
    "# Total Test\n",
    "# create the sentiment columns\n",
    "total_test_row_df = create_sentiment_columns(total_test_row_df)\n",
    "# create the emotion columns\n",
    "total_test_row_df = create_emotion_columns(total_test_row_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2737687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "emotion_train_row_df.to_json('msvd_train1k.json', orient='records', lines=False)\n",
    "# total test\n",
    "total_test_row_df.to_json('didemo_test400.json', orient='records', lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa129731",
   "metadata": {},
   "source": [
    "# 2. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60134dd9-ba72-464a-827b-eafe8d6e8589",
   "metadata": {},
   "source": [
    "## 2.1 Baseline (CLIP-ViP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35ef2b-c8a1-42a2-8645-8f6f1e5ca6ac",
   "metadata": {},
   "source": [
    "<img src=\"./baseline.png\" alt=\"nn\" width=\"1000\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d7cc7-6358-4c3c-a5a4-25e89873f44a",
   "metadata": {},
   "source": [
    "    - Task: video-language alignment (Video-to-Text, Text-to-Video)\n",
    "    - Goal: \n",
    "        - Adapting image-text pre-trained models to video-text pre-training (i.e. post-training) \n",
    "        - Video Proxy mechanism on the basis of CLIP (CLIP-ViP) \n",
    "    - Approach \n",
    "        - In-domain auxiliary data generation: \n",
    "            - to bridge language domain gaps between images and videos datasets \n",
    "            - introduce auxiliary captions into large-scale video-subtitle data to reduce the language domain gap between pre-training and downstream data \n",
    "                - pre-training: Image-Text learning \n",
    "                - downstream data: Video-Text learning \n",
    "\n",
    "        - Video Proxy Mechanism:\n",
    "            - to enable the Vision Transformer (ViT) model for both image and video encoding \n",
    "            - Before feeding into CLIP, we concatenate path tokens with a set of learnable parameters called video proxy tokens\n",
    "            - The output of the first video proxy token will be regarded as the video's representation \n",
    "    - Loss function:\n",
    "        - info-NCE loss \n",
    "\n",
    "\n",
    "\n",
    "We set the CLIP-ViP(1) model as our baseline, which is a video-text retrieval model based on pretrained image-text CLIP models. The main overview of the model is outlined above, which mainly focuses on utilizing CLIP models to work on video-text retrieval tasks through \"post-training\", and using other novel methods like in-domain auxiliary data generation and the video proxy mechanism. Like many other video-text retrieval models, it uses contrastive learning methods to better do perform task. \n",
    "\n",
    "\n",
    "(1) Xue, Hongwei, et al. \"Clip-vip: Adapting pre-trained image-text model to video-language alignment.\" The Eleventh International Conference on Learning Representations. 2022.\n",
    "## 2.2 Our Model (Use of Emotion Embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffbc376-9b3d-48e4-992d-226ada60a46f",
   "metadata": {},
   "source": [
    "<img src=\"./ourEmbeddingModel_2.png\" alt=\"nn\" width=\"1000\" height=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663b2ad",
   "metadata": {},
   "source": [
    "\n",
    "    - Modify the creation of text embeddings in the CLIP-ViP model\n",
    "        - Use the emotion features extracted in the dataset as \"emotion embeddings\"\n",
    "        - We find the average of the emotion embeddings corresponding to the emotions in each caption\n",
    "        - This average emotion embedding is added to the final text embedding to allow the model to incorporate emotion data\n",
    "Below, we give an example in code, of the modified Text Embedding class of the original model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fbd65-33e7-49a9-843f-082caa3798f4",
   "metadata": {},
   "source": [
    "### Model Code - Emotion Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3f198bc-3f50-4eba-968a-947db24cbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7410021-66f0-4b0f-b832-2df37fe742af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\n",
    "\"video\": \"video7112.mp4\",\n",
    "\"video_id\": \"video7112\",\n",
    "\"caption\": \"while other friends too try and hitting the basket another is eager to achieve his fourth successful basket in basketball\",\n",
    "\"duration\": 18.35,\n",
    "\"emotion\": 1,\n",
    "\"positive\": 4.0,\n",
    "\"negative\": 0.0,\n",
    "\"neutral\": 0.0,\n",
    "\"joy\": 4.0,\n",
    "\"trust\": 3.0,\n",
    "\"surprise\": 1.0,\n",
    "\"anticipation\": 3.0,\n",
    "\"fear\": 0.0,\n",
    "\"sadness\": 0.0,\n",
    "\"disgust\": 0.0,\n",
    "\"anger\": 0.0\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb501e29-89af-43ac-8888-0d9abb2f80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings for each of the 8 emotions\n",
    "embed_dim = 512\n",
    "emotion_embedding = nn.Embedding(8, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf737c3-85cb-48cb-b1d4-b1e76f604f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(8, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding table with embeddings for each of the 8 emotions\n",
    "emotion_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ddaa3de-04a8-43bb-9047-678829498ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "emotions: tensor([[1., 1., 1., 1., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change non-zero values to 1, effectively binarizing the input\n",
    "emotions = torch.tensor(np.array([4.0, 3.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0])).unsqueeze(0) \n",
    "print(emotions.shape) # (bs, 8)\n",
    "if emotions is not None:\n",
    "    emotions = torch.where(emotions > 0, torch.ones_like(emotions), torch.zeros_like(emotions))\n",
    "    print(f\"emotions: {emotions}\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9cc6678-47a9-4d5d-8d76-c8657b802794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all emotion embeddings\n",
    "batch_size = 1 \n",
    "all_emotion_embeds = emotion_embedding.weight.unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size, 8, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78b51f96-b1b6-4be9-a0d2-be1aae53e04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_emotion_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b48b6362-506a-4306-a52f-afc1e57cf248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion_mask: tensor([[[1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]), shape = torch.Size([1, 8, 1])\n",
      "\n",
      "selected_motion_embeds: torch.Size([1, 8, 512])\n",
      "\n",
      "emotion_embeds.shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# We find the emotion embeddings for the emotions that are present in the input\n",
    "if emotions is not None:\n",
    "    emotion_mask = emotions.unsqueeze(-1).type_as(all_emotion_embeds)  # [batch_size, 8, 1]\n",
    "    print(f\"emotion_mask: {emotion_mask}, shape = {emotion_mask.shape}\\n\")\n",
    "    selected_emotion_embeds = all_emotion_embeds * emotion_mask  # [batch_size, 8, embed_dim]\n",
    "    print(f\"selected_motion_embeds: {selected_emotion_embeds.shape}\\n\")\n",
    "    \n",
    "    # We calculate the average of the emotion embeddings that are present\n",
    "    safe_divisor = emotion_mask.sum(1) + (emotion_mask.sum(1) == 0).type_as(emotion_mask)\n",
    "    emotion_embeds = selected_emotion_embeds.sum(1) / safe_divisor # [batch_size, embed_dim]\n",
    "\n",
    "    print(f\"emotion_embeds.shape: {emotion_embeds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab9731-0ad7-43ef-baaf-d87a796d8b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = ? \n",
    "\n",
    "# Expand the emotion embeddings to match the sequence length\n",
    "emotion_embeds = emotion_embeds.unsqueeze(1).expand(-1, seq_length, -1)  # [batch_size, seq_length, embed_dim]\n",
    "\n",
    "position_embeddings = self.position_embedding(position_ids)\n",
    "\n",
    "# Final text embeddings are the sum of the input embeddings, position embeddings and emotion embeddings\n",
    "embeddings = inputs_embeds + position_embeddings + emotion_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05488bc0",
   "metadata": {},
   "source": [
    "# 3. Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba3268",
   "metadata": {},
   "source": [
    "## 3.1 Training Loss Curves and Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1dd79f",
   "metadata": {},
   "source": [
    "Performance\n",
    "1. MSR-VTT 9K: baseline\n",
    "    - Training Loss\n",
    "    - Validation Performance\n",
    "2. MSR-VTT 7K: baseline\n",
    "    - Training Loss\n",
    "    - Validation Performance\n",
    "3. MSR-VTT 6K: baseline\n",
    "    - Training Loss\n",
    "    - Validation Performance\n",
    "4. MSR-VTT 6K: emotion embedding\n",
    "    - Training Loss\n",
    "    - Validation Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eb21a9",
   "metadata": {},
   "source": [
    "## 3.2 Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52a1c2",
   "metadata": {},
   "source": [
    "To first evaluate the effects of training on different size datasplits, we trained the baseline model on the conventional 7k and 9k training sets and validated on the 1k validation set. In addition, we also conducted training on the 6k-emotion dataset we previously created. To evaluate the overall performance of the models, we first evaluate their performance on the entire test set, labeled \"Emotion+Neutral\" in the tables. On the other hand, __to evaluate the performance of these models on the retrieval of the caption-video pairs with emotions, which we previously identified as 34 videos out of the 1000 videos in the test dataset, we calculate the recall values for only these 34 queries, instead of the total 1000.__ These results are listed in the columns labeled \"Emotion\" in the tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad70c44",
   "metadata": {},
   "source": [
    "## MSR-VTT 7k: Baseline Model\n",
    "\n",
    "\n",
    "| Test Data                 | Emotion+Neutral |  Emotion+Neutral          |  Emotion  | Emotion          |\n",
    "|:---------------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n",
    "| Metric                    | T2V          | V2T          | T2V          | V2T          |\n",
    "| Recall@1               |   49.4000%   |  47.8044%    |  32.3529%  |  41.1765%  |\n",
    "| Recall@5               |   73.0000%   |  74.9501%    |  61.7647%  |  67.6471%  |\n",
    "| Recall@10              |   83.4000%   |   84.4311%   |  73.5294%  |  82.3529%  |\n",
    "| Recall Median          |     2.0         |   2.0        |    3.5     |    3.0     |\n",
    "| Recall Mean            |       14.5       |   10.3       |   18.1     |   14.6     |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed732a",
   "metadata": {},
   "source": [
    "## MSR-VTT 9k: Baseline Model\n",
    "\n",
    "\n",
    "| Test Data                 | Emotion+Neutral |  Emotion+Neutral          |  Emotion  | Emotion          |\n",
    "|:---------------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n",
    "| Metric                    | T2V          | V2T          | T2V          | V2T          |\n",
    "| Recall@1               |  49.5000%    | 49.3028%     |  35.2941%  |  35.2941%  |\n",
    "| Recall@5               |   74.7000%   | 76.6932%     |  61.7647%  |  67.6471%  |\n",
    "| Recall@10              |  84.8000%    | 85.3586%     |  73.5294%  |  82.3529%  |\n",
    "| Recall Median          |   2.0        |    2.0       |    2.5     |    3.0     |\n",
    "| Recall Mean            |   13.4       |    9.5       |   15.9     |   13.1     |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0529c400",
   "metadata": {},
   "source": [
    "## MSR-VTT 6k (Emotion): Baseline Model\n",
    "| Test Data                 | Emotion+Neutral |  Emotion+Neutral          |  Emotion  | Emotion          |\n",
    "|:---------------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n",
    "| Metric                    | T2V          | V2T          | T2V          | V2T          |\n",
    "| Recall@1               |   49.0000%   |  48.4032%    |  29.4118%  |  32.3529%  |\n",
    "| Recall@5               |    73.2000%  |  75.6487%    |  58.8235%  |  70.5882%  |\n",
    "| Recall@10              |  84.5000%    |  84.7305%    |  79.4118%  |  79.4118%  |\n",
    "| Recall Median          |   2.0        |   2.0        |    3.5     |    2.0     |\n",
    "| Recall Mean            |    13.6      |   9.9        |   16.8     |   14.7     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3129607c",
   "metadata": {},
   "source": [
    "As expected, in regard to the model's performance on emotion-containing queries in the \"Emotion\" columns, the performance degrades on the 6k(emotion) dataset in comparison to the other two datasets with both emotional and neutral data, as the model sees less data during training. From this, we conclude that training exclusively on data containing only emotion does not translate to improved performance on emotion-containing queries. In addition, when comparing the results for the entire test set with only the emotion-containing queries, we find that for all training set sizes, the recall values for \"Emotion\" are all lower than for \"Emotion+Neutral\". We find these as reasons to additionally implement methods to focus on the emotion information within our data, to better find such matches. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0aa5a",
   "metadata": {},
   "source": [
    "## MSR-VTT 6k (Emotion): Our Model (Emotion Embeddings)\n",
    "| Test Data                 | Emotion+Neutral |  Emotion+Neutral          |  Emotion  | Emotion          |\n",
    "|:---------------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n",
    "| Metric                    | T2V          | V2T          | T2V          | V2T          |\n",
    "| Recall@1                  |  23.9000%    |    44.8104%  | 5.8824%      | 8.8235%      |\n",
    "| Recall@5                  |  41.5000%    |  28.1437%    | 14.7059%     | 20.5882%     |\n",
    "| Recall@10                 |      49.00%  |   50.9980%   | 23.5294%     | 20.5882%     |\n",
    "| Recall Median             |     11.0     |   9.5        | 83.0         | 66.5         |\n",
    "| Recall Mean               |      124.4   |      72.6    | 206.6        | 115.5        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901d956",
   "metadata": {},
   "source": [
    "We show the results of training our proposed model. We find that our naive use of emotion embeddings actually hurts performance, with the recall values decreasing over the baseline. We find this can be caused by a variety of reasons. \n",
    "* The emotion embeddings are currently added to all tokens in the sequence, even for the padding used to fit the max length. We suspect that this can be adding noise during training.\n",
    "* The simple addition of these emotion embeddings may not be enough to capture the complex relationships they have with the captions and videos. More sophisticated methods should be explored. \n",
    "* We currently do not model the lack of emotions as an embedding, which is an important aspect of our data.\n",
    "\n",
    "These are all points that can be further improved during the remaining duration for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0880c10-4b3b-4198-956d-6130826210fa",
   "metadata": {},
   "source": [
    "# 4. Remaining Project Plan (To-Do) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587bf69",
   "metadata": {},
   "source": [
    "* We believe that an in-depth analysis is required to understand the exact effects of additionally integrating this emotion data during training. We will track the effects of changes in model architecture on the embedding space, by using visualization approaches such as t-SNE. We will choose our final model based on these observations, so that they best align with our goals.\n",
    "\n",
    "* As the goal of this project is to create and use a model that can better find videos more relevant to the query, especially emotionally, we will conduct further analysis on how the model learns to incorporate these emotions. We will do this by identifying semantically/emotionally similar captions to a previously unseen emotional query in advance, then track the changes in rankings for these captions when given this query as input. \n",
    "\n",
    "* Finally, as our main objective is to create a useful tool that users can use to retrieve more emotionally relevant videos based on their queries, we will build the functional application that utilizes this model to perform this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709cb46-fbe4-4621-9b43-174771c783ea",
   "metadata": {},
   "source": [
    "# 5. Possible Future Directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6531b",
   "metadata": {},
   "source": [
    "In this section, we show a list of the things we hope attempt/achieve during the remainder of the duration of this projection. We find there are many different aspects from which our project can be improved, and aim to implement as many of these changes as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8407c-4d8b-4b33-a6c8-1e4177148b7a",
   "metadata": {},
   "source": [
    "1. Test Data Collection:\n",
    "    - Image-captioning model OFA-Caption (Wang et al., 2022b) to generate one caption for the middle frame of each video in HD-VILA-100M\n",
    "          - with max length of 16 words\n",
    "2. Model development\n",
    "    - Our model currently uses the addition operator to aggregate the emotion embeddings in the text embeddings. Considering the nature of such sequential input, this may not be the most effective method to apply the emotion embeddings. We hope to use instead of a simple sum, a more sophisticated method such as concatenating the emotion embeddings and then sending them through a fully connected layer to learn optimal final emotion embeddings, or change the way we encode emotions as embeddings by further creating embeddings for captions without emotions, and several more modeling variations to improve performance. \n",
    "\n",
    "    - By creating a new emotion encoder, we hope to apply cross-attention on video, text, and emotion features to better learn the relationships between the three. (1) \n",
    "    \n",
    "    - In addition to the use of emotional data in the text(captions), we hope to utilize Facial Expression Recognition(FER) models to extract emotional data from the video data to integrate such video emotion features as well. \n",
    "3. Data Processing\n",
    "   - Lack of training and test data:\n",
    "         - As the number of videos filtered by the first step of data preprocessing, sentiment analysis, is so large, we find that we are short on data to train on. We plan to experiment with using the full dataset alongside the assigned emotions from step 2 of the data preprocessing process, essentially skipping step 1. \n",
    "\n",
    "(1) X. Zhang, M. Li, S. Lin, H. Xu and G. Xiao, \"Transformer-based Multimodal Emotional Perception for Dynamic Facial Expression Recognition in the Wild,\" in IEEE Transactions on Circuits and Systems for Video Technology, doi: 10.1109/TCSVT.2023.3312858.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ccc67-74ac-41d4-8e64-1be88a9ac9e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
